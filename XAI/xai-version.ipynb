{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3151739,"sourceType":"datasetVersion","datasetId":1915180},{"sourceId":11637761,"sourceType":"datasetVersion","datasetId":7302140}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# XAI Analysis for Handwritten Signature Verification\n\nThis notebook performs Explainable AI (XAI) analysis on a trained model for handwritten signature verification. The goal is to understand which parts of the input image the model focuses on when making predictions (Real vs. Forged signatures).\n\n## XAI Methods Used\n- **Grad-CAM**: Highlights regions in the image that contribute most to the model's prediction by using gradients from the final convolutional layer.\n- **LIME**: Identifies important superpixels in the image by perturbing the input and observing changes in the prediction.\n- **SHAP**: Assigns importance scores to each pixel by evaluating the model's output with and without certain pixels, using a background dataset.\n\n## Steps\n1. Install necessary libraries\n2. Import libraries and define the model class\n3. Configure paths and settings\n4. Load the trained model and input image\n5. Generate explanations using Grad-CAM, LIME, and SHAP\n6. Visualize the explanations in a combined plot","metadata":{}},{"cell_type":"markdown","source":"## --- 1. Install necessary libraries ---","metadata":{}},{"cell_type":"code","source":"print(\"--- Installing XAI libraries ---\")\n!pip install lime shap --quiet\n!pip install scikit-image --quiet # Needed by LIME\n!pip install opencv-python-headless --quiet # For cv2 used in visualizations\n!pip install -U albumentations --quiet\n!pip install grad-cam==1.4.8 --quiet # Installing pytorch-grad-cam\nprint(\"--- Installations complete ---\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:52:42.375681Z","iopub.execute_input":"2025-05-01T19:52:42.375968Z","iopub.status.idle":"2025-05-01T19:53:05.216851Z","shell.execute_reply.started":"2025-05-01T19:52:42.375944Z","shell.execute_reply":"2025-05-01T19:53:05.214937Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## --- 2. Imports and Model Class Definition ---","metadata":{}},{"cell_type":"markdown","source":"### --- Importing libraries ---","metadata":{"execution":{"iopub.status.busy":"2025-05-01T14:48:28.759581Z","iopub.execute_input":"2025-05-01T14:48:28.759977Z","iopub.status.idle":"2025-05-01T14:48:28.765006Z","shell.execute_reply.started":"2025-05-01T14:48:28.759953Z","shell.execute_reply":"2025-05-01T14:48:28.763974Z"}}},{"cell_type":"code","source":"print(\"--- Importing libraries ---\")\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms # For basic tensor conversion if needed\nimport timm\nimport os\nimport numpy as np\nfrom PIL import Image, UnidentifiedImageError\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport cv2 # For visualization\nimport time\nimport copy\nimport logging\nimport sys\nimport json\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm.auto import tqdm # For progress bars\nimport random # For random debug logging in CustomModel \nimport matplotlib.cm as cm # For colormaps\n\n# XAI Libraries\nimport lime\nfrom lime import lime_image\nimport shap\nfrom skimage.segmentation import mark_boundaries # For LIME visualization","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:53:05.218736Z","iopub.execute_input":"2025-05-01T19:53:05.219567Z","iopub.status.idle":"2025-05-01T19:53:13.556360Z","shell.execute_reply.started":"2025-05-01T19:53:05.219498Z","shell.execute_reply":"2025-05-01T19:53:13.555364Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### --- Setting up Logging ---","metadata":{}},{"cell_type":"code","source":"print(\"--- Setting up logging ---\")\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - [%(levelname)s] - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    handlers=[logging.StreamHandler(sys.stdout)],\n    force=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:53:13.557580Z","iopub.execute_input":"2025-05-01T19:53:13.558167Z","iopub.status.idle":"2025-05-01T19:53:13.564649Z","shell.execute_reply.started":"2025-05-01T19:53:13.558139Z","shell.execute_reply":"2025-05-01T19:53:13.563284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### --- Defining CustomModel Class  ---","metadata":{}},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    \"\"\"\n    Flexible model wrapper using timm or HuggingFace Transformers for the base\n    and adding a custom classifier head. Allows selective unfreezing of base layers.\n    Includes automatic feature dimension verification and adaptation.\n    \"\"\"\n    def __init__(self, model_name, dense_units, dropout, pretrained=True, unfreeze_layers=0):\n        super(CustomModel, self).__init__()\n        self.model_name = model_name\n        self.base_model = None\n        reported_features = 0  # Features reported by the base model\n\n        logger.info(f\"Initializing CustomModel: Base='{model_name}', DenseUnits={dense_units}, Dropout={dropout:.2f}, Unfreeze={unfreeze_layers}\")\n\n        try:\n            # --- Load Base Model ---\n            if model_name == \"ViT_Base\":\n                hf_model_name = 'google/vit-base-patch16-224'\n                # Load without pretrained weights initially if loading state_dict later\n                self.base_model = ViTModel.from_pretrained(\n                    hf_model_name,\n                    add_pooling_layer=False,\n                    ignore_mismatched_sizes=True\n                )\n                reported_features = self.base_model.config.hidden_size\n            else:\n                # Use timm for other models\n                timm_model_name_map = {\n                    \"MobileNetV3_Large\": \"mobilenetv3_large_100.miil_in21k_ft_in1k\",\n                }\n                if model_name not in timm_model_name_map:\n                    raise ValueError(f\"Model name '{model_name}' not found in timm map or not supported.\")\n\n                timm_name = timm_model_name_map[model_name]\n                \n                # Load structure only, weights will come from state_dict\n                self.base_model = timm.create_model(timm_name, pretrained=False, num_classes=0)\n                # Determine features from forward pass\n                self.base_model.eval()\n                with torch.no_grad():\n                    dummy_input = torch.randn(1, 3, 224, 224)\n                    features = self.base_model(dummy_input)\n                    reported_features = features.shape[1]\n                    \n                    logger.info(f\"Loaded '{timm_name}' structure from timm. Reported features: {reported_features}\")\n\n            # --- Determine Feature Dimensions (Simplified - assuming trained model worked) ---\n            # For XAI, we rely on the feature dimension being correct from training\n            num_features = reported_features # Use the reported dimension\n\n            # --- Parameter Freezing/Unfreezing (NOT needed when loading state_dict) ---\n            # The requires_grad status is not saved in the state_dict.\n            # We don't need to freeze/unfreeze here; just build the matching structure.\n\n            # --- Define Classifier Head ---\n            self.classifier = nn.Sequential(\n                nn.Linear(num_features, dense_units),\n                nn.ReLU(),\n                nn.BatchNorm1d(dense_units), # BatchNorm is important\n                nn.Dropout(dropout),\n                nn.Linear(dense_units, 1),  # Output 1 logit for binary classification\n            )\n\n        except Exception as e:\n            logger.error(f\"Error initializing model structure '{model_name}': {e}\", exc_info=True)\n            raise\n\n    def forward(self, x):\n        if self.model_name == \"ViT_Base\":\n            features = self.base_model(x).last_hidden_state[:, 0]\n        else:\n            features = self.base_model(x)\n        output = self.classifier(features)\n        return output\n\nprint(\"--- Library imports and Model class definition complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:53:13.565864Z","iopub.execute_input":"2025-05-01T19:53:13.566310Z","iopub.status.idle":"2025-05-01T19:53:13.592249Z","shell.execute_reply.started":"2025-05-01T19:53:13.566277Z","shell.execute_reply":"2025-05-01T19:53:13.591128Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### --- Helper Functions ---","metadata":{}},{"cell_type":"markdown","source":"### --- Helper Function for Heatmap Generation ---","metadata":{}},{"cell_type":"code","source":"def generate_heatmap_overlay(image, heatmap_data, colormap='viridis', alpha=0.6):\n    \"\"\"\n    Generates a heatmap overlay on an image.\n\n    Args:\n        image (np.ndarray): Base image (H, W, C) uint8 [0, 255] or float [0, 1].\n        heatmap_data (np.ndarray): Heatmap weights (H, W), normalized preferred.\n        colormap (str): Name of the matplotlib colormap.\n        alpha (float): Transparency of the heatmap overlay.\n\n    Returns:\n        np.ndarray: Image with heatmap overlay.\n    \"\"\"\n    if image.dtype == np.float32 or image.dtype == np.float64:\n        image = (image * 255).astype(np.uint8) # Convert to uint8 if float\n\n    if image.ndim == 2: # Grayscale image? Convert to BGR for overlay\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n    elif image.shape[2] == 3 and image.dtype == np.uint8:\n         # Ensure it's BGR if it came directly from PIL/OpenCV load\n         # If it came from matplotlib display, it might be RGB. Assume BGR typical for cv2 ops.\n         # If colors look swapped later, convert: image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n         pass # Assuming BGR or compatible\n    elif image.shape[2] == 4: # RGBA\n         image = cv2.cvtColor(image, cv2.COLOR_RGBA2BGR)\n\n\n    # Normalize heatmap data to 0-1 if not already\n    if np.nanmax(heatmap_data) > 1.0 or np.nanmin(heatmap_data) < 0.0:\n        heatmap_data = np.clip(heatmap_data, 0, np.nanmax(heatmap_data)) # Clip negative for positive-only heatmaps\n        max_val = np.nanmax(heatmap_data)\n        if max_val > 1e-6: # Avoid division by zero\n             heatmap_data = heatmap_data / max_val\n        else:\n             heatmap_data = np.zeros_like(heatmap_data) # Set to zero if max is near zero\n\n    # Get colormap function\n    cmap = matplotlib.colormaps.get_cmap(colormap)\n    # Apply colormap (returns RGBA float 0-1)\n    heatmap_colored = cmap(heatmap_data)[:, :, :3] # Take only RGB\n    heatmap_colored = (heatmap_colored * 255).astype(np.uint8) # Convert to uint8\n\n    # Ensure heatmap is BGR if image is BGR\n    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_RGB2BGR)\n\n    # Blend the heatmap and the original image\n    # cv2.addWeighted requires same size and type\n    if image.shape != heatmap_colored.shape:\n         # This shouldn't happen if heatmap_data was (H, W) and image was (H, W, 3)\n         logger.error(f\"Shape mismatch: Image {image.shape}, Heatmap {heatmap_colored.shape}. Cannot overlay.\")\n         return image # Return original image on error\n\n    overlay = cv2.addWeighted(image, 1 - alpha, heatmap_colored, alpha, 0)\n\n    return overlay","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:53:13.593792Z","iopub.execute_input":"2025-05-01T19:53:13.594172Z","iopub.status.idle":"2025-05-01T19:53:13.618248Z","shell.execute_reply.started":"2025-05-01T19:53:13.594140Z","shell.execute_reply":"2025-05-01T19:53:13.617118Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### --- Helper Function to Replace In-Place Hardswish ---","metadata":{}},{"cell_type":"code","source":"def replace_hardswish(module):\n    \"\"\"\n    Recursively replace all nn.Hardswish modules with inplace=False.\n    \"\"\"\n    for name, child in module.named_children():\n        if isinstance(child, nn.Hardswish):\n            setattr(module, name, nn.Hardswish(inplace=False))\n            # logger.info(f\"Replaced Hardswish at {name} with inplace=False\")\n        else:\n            replace_hardswish(child)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:53:13.619194Z","iopub.execute_input":"2025-05-01T19:53:13.619972Z","iopub.status.idle":"2025-05-01T19:53:13.644092Z","shell.execute_reply.started":"2025-05-01T19:53:13.619443Z","shell.execute_reply":"2025-05-01T19:53:13.642863Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### --- Helper Function to Load Images ---","metadata":{}},{"cell_type":"code","source":"# --- Helper Function to Load Images ---\ndef load_image(image_path, transform_type='tensor'):\n    \"\"\"\n    Load and preprocess an image.\n    \n    Args:\n        image_path (str): Path to the image.\n        transform_type (str): 'tensor' for model input, 'numpy' for LIME/SHAP.\n    \n    Returns:\n        torch.Tensor or np.ndarray: Preprocessed image.\n    \"\"\"\n    try:\n        image = Image.open(image_path).convert('RGB')\n        logger.info(f\"Loaded image: {image_path}, size: {image.size}\")\n    except UnidentifiedImageError as e:\n        logger.error(f\"Cannot identify image file {image_path}: {e}\")\n        raise\n    \n    if transform_type == 'tensor':\n        transform = transforms.Compose([\n            transforms.Resize(IMG_SIZE),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        image_tensor = transform(image)\n        logger.info(f\"Transformed image to tensor: shape={image_tensor.shape}\")\n        return image_tensor\n    else:  # 'numpy' for LIME/SHAP\n        image_np = np.array(image)\n        transform = A.Compose([\n            A.Resize(*IMG_SIZE),\n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n        augmented = transform(image=image_np)\n        image_np = augmented['image']\n        logger.info(f\"Transformed image to numpy: shape={image_np.shape}\")\n        return image_np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:53:13.646865Z","iopub.execute_input":"2025-05-01T19:53:13.647219Z","iopub.status.idle":"2025-05-01T19:53:13.667011Z","shell.execute_reply.started":"2025-05-01T19:53:13.647194Z","shell.execute_reply":"2025-05-01T19:53:13.665600Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## --- 3. Configuration ---","metadata":{}},{"cell_type":"markdown","source":"### --- Model Configuration ---","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"MobileNetV3_Large\"    \n# Hyperparameters from the Optuna results \nDENSE_UNITS = 768 \nDROPOUT = 0.45     \nUNFREEZE_LAYERS = 3 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:53:13.668294Z","iopub.execute_input":"2025-05-01T19:53:13.668606Z","iopub.status.idle":"2025-05-01T19:53:13.694780Z","shell.execute_reply.started":"2025-05-01T19:53:13.668576Z","shell.execute_reply":"2025-05-01T19:53:13.693663Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### --- Paths --- ","metadata":{}},{"cell_type":"code","source":"MODEL_PATH = \"/kaggle/input/checkpoints-xai/MobileNetV3_Large_best_val_loss.pth\"\nIMAGE_PATH = \"/kaggle/input/handwritten-signature-verification/data/data/forged/52F192AB-A654-4778-861A-C81E555D4656.jpg/2__52F192AB-A654-4778-861A-C81E555D4656.jpg.jpg\"\nOUTPUT_DIR = \"/kaggle/working/xai_outputs\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# SHAP Background Data\nBACKGROUND_DATA_DIR = \"/kaggle/input/handwritten-signature-verification/data/data/real\" # Using 'real' signatures as background\nN_SHAP_BACKGROUND_SAMPLES = 50 # Number of background images for SHAP","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:53:13.695867Z","iopub.execute_input":"2025-05-01T19:53:13.696218Z","iopub.status.idle":"2025-05-01T19:53:13.715504Z","shell.execute_reply.started":"2025-05-01T19:53:13.696195Z","shell.execute_reply":"2025-05-01T19:53:13.714102Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### --- Other Settings ---","metadata":{"execution":{"iopub.status.busy":"2025-05-01T13:35:11.611962Z","iopub.execute_input":"2025-05-01T13:35:11.612452Z","iopub.status.idle":"2025-05-01T13:35:11.624295Z","shell.execute_reply.started":"2025-05-01T13:35:11.612396Z","shell.execute_reply":"2025-05-01T13:35:11.623186Z"}}},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlogger.info(f\"Using device: {DEVICE}\")\n\n# Image size \nmodel_img_sizes = {\n    \"MobileNetV3_Large\": (224, 224)\n}\n\nif MODEL_NAME not in model_img_sizes:\n    raise ValueError(f\"Image size for model '{MODEL_NAME}' not defined in 'model_img_sizes'.\")\nIMG_SIZE = model_img_sizes[MODEL_NAME]\nlogger.info(f\"Using Image Size: {IMG_SIZE} for model {MODEL_NAME}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:53:13.716682Z","iopub.execute_input":"2025-05-01T19:53:13.717203Z","iopub.status.idle":"2025-05-01T19:53:13.738651Z","shell.execute_reply.started":"2025-05-01T19:53:13.717145Z","shell.execute_reply":"2025-05-01T19:53:13.736704Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## --- 4. Load Model and Image ---","metadata":{}},{"cell_type":"markdown","source":"### --- Load the Trained Model ---","metadata":{}},{"cell_type":"code","source":"logger.info(f\"--- Loading model structure for: {MODEL_NAME} ---\")\ntry:\n    model = CustomModel(\n        model_name=MODEL_NAME,\n        dense_units=DENSE_UNITS,\n        dropout=DROPOUT,\n        pretrained=False, # Weights loaded from state_dict\n        unfreeze_layers=UNFREEZE_LAYERS \n    )   \n    logger.info(f\"Model structure created successfully.\")\n\n    logger.info(f\"--- Loading state dict from: {MODEL_PATH} ---\")\n    if not os.path.exists(MODEL_PATH):\n         raise FileNotFoundError(f\"Model file not found at {MODEL_PATH}\")\n\n    # Load state dict \n    state_dict = torch.load(MODEL_PATH, map_location=DEVICE, weights_only=True)\n\n    # Handle potential keys mismatch\n    if all(key.startswith('module.') for key in state_dict.keys()):\n        logger.info(\"Removing 'module.' prefix from state_dict keys.\")\n        state_dict = {k.replace('module.', '', 1): v for k, v in state_dict.items()}\n\n    # Load the state dictionary\n    model.load_state_dict(state_dict)\n    logger.info(\"State dict loaded successfully.\")\n    \n    # Modify Hardswish to non-inplace\n    if MODEL_NAME == \"MobileNetV3_Large\":\n        logger.info(\"Replacing all in-place Hardswish activations in MobileNetV3_Large.\")\n        replace_hardswish(model.base_model)\n\n    model.to(DEVICE)\n    model.eval() # Set model to evaluation mode\n    logger.info(\"Model loaded and set to evaluation mode.\")\n\nexcept Exception as e:\n    logger.error(f\"Failed to load model: {e}\", exc_info=True)\n    raise # Stop execution if model loading fails","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:53:13.739724Z","iopub.execute_input":"2025-05-01T19:53:13.740174Z","iopub.status.idle":"2025-05-01T19:53:14.416045Z","shell.execute_reply.started":"2025-05-01T19:53:13.740147Z","shell.execute_reply":"2025-05-01T19:53:14.415110Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### --- Load the target image ---","metadata":{}},{"cell_type":"code","source":"logger.info(f\"--- Loading target image for explanation: {IMAGE_PATH} ---\")\ntarget_image_tensor = load_image(IMAGE_PATH, transform_type='tensor')\ntarget_image_numpy = load_image(IMAGE_PATH, transform_type='numpy') # For LIME visualization\nlogger.info(\"Target image loaded successfully.\")\n\nif target_image_tensor is None or target_image_numpy is None:\n    logger.error(\"Failed to load the target image. Exiting.\")\n    exit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:53:14.417448Z","iopub.execute_input":"2025-05-01T19:53:14.417906Z","iopub.status.idle":"2025-05-01T19:53:14.452434Z","shell.execute_reply.started":"2025-05-01T19:53:14.417867Z","shell.execute_reply":"2025-05-01T19:53:14.451188Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## --- 5. Explanation Generation ---","metadata":{}},{"cell_type":"markdown","source":"### --- Grad-CAM Explanation ---","metadata":{}},{"cell_type":"code","source":"logger.info(\"--- Starting Grad-CAM Explanation (using pytorch-grad-cam) ---\")\nstart_gradcam_time = time.time()\ntry:\n    from pytorch_grad_cam import GradCAM\n    from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n\n    # Define a custom target for binary classification with single logit output\n    class BinaryClassifierOutputTarget:\n        def __init__(self, target_class):\n            self.target_class = target_class  # 0 for \"Real\", 1 for \"Forged\"\n\n        def __call__(self, model_output):\n            # model_output is [batch_size, 1] (sibngle logit)\n            if self.target_class == 1:  # \"Forged\" -> maximize the sigmoid output\n                return model_output  # Gradient w.r.t. the logit directly\n            else:  # \"Real\" -> minimize the sigmoid output (maximize 1 - sigmoid)\n                return -model_output  # Negate the logit to compute gradient for \"Real\"\n\n    # Prepare input and predict class\n    input_tensor = target_image_tensor.unsqueeze(0).to(DEVICE)\n    with torch.no_grad():\n        output = model(input_tensor)\n        output_prob = torch.sigmoid(output).item()\n    predicted_class_idx = 1 if output_prob > 0.5 else 0\n    predicted_class_name = \"Forged\" if predicted_class_idx == 1 else \"Real\"\n    logger.info(f\"Predicted class: {predicted_class_name} (Probability: {output_prob:.4f})\")\n\n    # Select target layer for MobileNetV3_Large\n    target_layers = [model.base_model.blocks[2][0].conv_pwl]  # Earlier layer\n    grad_cam = GradCAM(model=model, target_layers=target_layers)\n\n    # Use the custom target for binary classification\n    targets = [BinaryClassifierOutputTarget(predicted_class_idx)]\n    cam = grad_cam(input_tensor=input_tensor, targets=targets)\n    cam = cam[0]  # Shape: (H, W)\n\n    gradcam_heatmap_overlay = generate_heatmap_overlay(\n        target_image_numpy,\n        cam,\n        colormap='jet',\n        alpha=0.6\n    )\n    gradcam_output_path = os.path.join(OUTPUT_DIR, f\"{os.path.basename(IMAGE_PATH).split('.')[0]}_gradcam.png\")\n    cv2.imwrite(gradcam_output_path, gradcam_heatmap_overlay)\n    logger.info(f\"Grad-CAM visualization saved to {gradcam_output_path}\")\nexcept Exception as e:\n    logger.error(f\"Error generating Grad-CAM with pytorch-grad-cam: {e}\", exc_info=True)\n    logger.info(\"Falling back to custom Grad-CAM...\")\n    try:\n        target_layers = [model.base_model.blocks[2][0].conv_pwl]  # Ensure it's a list\n        logger.info(\"Using target layer: blocks[2][0].conv_pwl\")\n        grad_cam = GradCAM(model=model, target_layers=target_layers)\n\n        # Use the same custom target for the fallback\n        targets = [BinaryClassifierOutputTarget(predicted_class_idx)]\n        cam = grad_cam(input_tensor=input_tensor, targets=targets)\n        cam = cam[0]  # Shape: (H, W)\n\n        gradcam_heatmap_overlay = generate_heatmap_overlay(\n            target_image_numpy,\n            cam,\n            colormap='jet',\n            alpha=0.6\n        )\n        gradcam_output_path = os.path.join(OUTPUT_DIR, f\"{os.path.basename(IMAGE_PATH).split('.')[0]}_gradcam_fallback.png\")\n        cv2.imwrite(gradcam_output_path, gradcam_heatmap_overlay)\n        logger.info(f\"Grad-CAM (fallback) visualization saved to {gradcam_output_path}\")\n    except Exception as e:\n        logger.error(f\"Error generating custom Grad-CAM: {e}\", exc_info=True)\n        gradcam_heatmap_overlay = target_image_numpy\n        gradcam_output_path = None\n\ngradcam_duration = time.time() - start_gradcam_time\nlogger.info(f\"Grad-CAM explanation generated in {gradcam_duration:.2f} seconds.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:53:14.453731Z","iopub.execute_input":"2025-05-01T19:53:14.454322Z","iopub.status.idle":"2025-05-01T19:53:14.730370Z","shell.execute_reply.started":"2025-05-01T19:53:14.454291Z","shell.execute_reply":"2025-05-01T19:53:14.729549Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### --- Grad-CAM visualization ---","metadata":{}},{"cell_type":"code","source":"logger.info(\"--- Displaying Grad-CAM Heatmap ---\")\ntry:\n    if gradcam_output_path and os.path.exists(gradcam_output_path):\n        # Load the saved Grad-CAM heatmap overlay\n        gradcam_img = cv2.imread(gradcam_output_path)\n        gradcam_img_rgb = cv2.cvtColor(gradcam_img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for matplotlib\n        \n        # Display the heatmap\n        plt.figure(figsize=(6, 6))\n        plt.imshow(gradcam_img_rgb)\n        plt.title(f\"Grad-CAM Heatmap (Predicted: {predicted_class_name})\")\n        plt.axis('off')\n        plt.show()\n        logger.info(\"Grad-CAM heatmap displayed successfully.\")\n    else:\n        logger.warning(\"Grad-CAM output path does not exist. Cannot display heatmap.\")\nexcept Exception as e:\n    logger.error(f\"Error displaying Grad-CAM heatmap: {e}\", exc_info=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:53:14.731291Z","iopub.execute_input":"2025-05-01T19:53:14.731570Z","iopub.status.idle":"2025-05-01T19:53:15.032673Z","shell.execute_reply.started":"2025-05-01T19:53:14.731550Z","shell.execute_reply":"2025-05-01T19:53:15.031449Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### --- LIME Explanation ---\n","metadata":{}},{"cell_type":"code","source":"logger.info(\"--- Starting LIME Explanation ---\")\nstart_lime_time = time.time()\ntry:\n    # Define prediction function for LIME\n    def predict_fn(images):\n        images_tensor = torch.stack([\n            transforms.ToTensor()(img) for img in images\n        ]).to(DEVICE)\n        # Normalize as per training\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        images_tensor = normalize(images_tensor)\n        model.eval()\n        with torch.no_grad():\n            outputs = model(images_tensor)\n            probs = torch.sigmoid(outputs).cpu().numpy()\n        return np.hstack([1 - probs, probs])  # [p(Real), p(Forged)]\n\n    explainer = lime_image.LimeImageExplainer()\n    explanation = explainer.explain_instance(\n        target_image_numpy,\n        predict_fn,\n        top_labels=2,\n        hide_color=0,\n        num_samples=1000\n    )\n\n    # Get LIME visualization for the predicted class\n    temp, mask = explanation.get_image_and_mask(\n        label=predicted_class_idx,\n        positive_only=True,\n        num_features=5,\n        hide_rest=False\n    )\n    lime_heatmap_overlay = mark_boundaries(target_image_numpy / 255.0, mask)\n    lime_heatmap_overlay = (lime_heatmap_overlay * 255).astype(np.uint8)\n\n    lime_output_path = os.path.join(OUTPUT_DIR, f\"{os.path.basename(IMAGE_PATH).split('.')[0]}_lime.png\")\n    cv2.imwrite(lime_output_path, cv2.cvtColor(lime_heatmap_overlay, cv2.COLOR_RGB2BGR))\n    logger.info(f\"LIME visualization saved to {lime_output_path}\")\nexcept Exception as e:\n    logger.error(f\"Error generating LIME explanation: {e}\", exc_info=True)\n    lime_heatmap_overlay = target_image_numpy\n    lime_output_path = None\n\nlime_duration = time.time() - start_lime_time\nlogger.info(f\"LIME explanation generated in {lime_duration:.2f} seconds.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:53:15.033896Z","iopub.execute_input":"2025-05-01T19:53:15.034215Z","iopub.status.idle":"2025-05-01T19:53:43.447647Z","shell.execute_reply.started":"2025-05-01T19:53:15.034192Z","shell.execute_reply":"2025-05-01T19:53:43.446721Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### --- LIME Visualization ---","metadata":{}},{"cell_type":"code","source":"# --- Visualize LIME results ---\n# We are interested in explaining the predicted class. Let's get the model's prediction first.\nwith torch.no_grad():\n    output_logit = model(target_image_tensor.unsqueeze(0).to(DEVICE))\n    output_prob = torch.sigmoid(output_logit).item()\npredicted_class_idx = 1 if output_prob > 0.5 else 0\npredicted_class_name = \"Forged\" if predicted_class_idx == 1 else \"Real\"\nlogger.info(f\"Model prediction: {predicted_class_name} (Probability: {output_prob:.4f})\")\n\n# Function to safely normalize image data for visualization\ndef normalize_for_display(img):\n    \"\"\"Normalize image to [0,1] range for safe display\"\"\"\n    img_min, img_max = np.min(img), np.max(img)\n    if img_min == img_max:\n        return np.zeros_like(img)\n    return (img - img_min) / (img_max - img_min)\n    \n# Get the LIME explanation mask for the *predicted* class\n# explanation_lime.top_labels[0] usually corresponds to the class with highest probability\n# Let's explicitly get the explanation for the predicted class index (0 or 1)\n\n# Create a figure with 3 subplots: original image, positive features, negative features\nplt.figure(figsize=(15, 5))\n    \n    # --- 1. LIME Boundary Plot ---\ntry:\n    temp_lime, mask_lime = explanation_lime.get_image_and_mask(\n        predicted_class_idx,\n        positive_only=True,\n        num_features=10,\n        hide_rest=False\n    )\n    \n    # Normalize temp_lime safely to avoid clipping warnings\n    temp_lime_norm = normalize_for_display(temp_lime)\n    \n    plt.subplot(1, 3, 1)\n    plt.imshow(mark_boundaries(temp_lime_norm, mask_lime))\n    plt.title(f\"LIME Boundaries (Pos for '{predicted_class_name}')\")\n    plt.axis('off')\n    show_boundaries = True\nexcept Exception as e:\n    logger.error(f\"Could not generate LIME boundary plot: {e}\", exc_info=True)\n    plt.subplot(1, 3, 1)\n    plt.imshow(target_image_numpy)  # Show original if boundaries fail\n    plt.title(\"Original Image (LIME Boundaries Failed)\")\n    plt.axis('off')\n    show_boundaries = False\n\n# --- 2. LIME Heatmap Generation ---\ntry:\n    # Create empty heatmap weights array\n    lime_heatmap_weights = np.zeros(explanation_lime.segments.shape)\n    \n    # Get weights for the predicted class\n    lime_exp = explanation_lime.local_exp[predicted_class_idx]\n    \n    # Map positive weights to the segments\n    for seg_id, weight in lime_exp:\n        if weight > 0:  # Only consider positive contributions\n            lime_heatmap_weights[explanation_lime.segments == seg_id] = weight\n    \n    # Normalize weights for better visualization\n    if np.max(lime_heatmap_weights) > 0:\n        lime_heatmap_weights = lime_heatmap_weights / np.max(lime_heatmap_weights)\n    \n    # Generate heatmap overlay\n    # Call the existing generate_heatmap_overlay function\n    lime_heatmap_overlay = generate_heatmap_overlay(\n        target_image_numpy,\n        lime_heatmap_weights,\n        colormap='viridis',  # Or 'hot', 'jet', 'Reds'\n        alpha=0.7\n    )\n    \n    plt.subplot(1, 3, 2)\n    plt.imshow(cv2.cvtColor(lime_heatmap_overlay, cv2.COLOR_BGR2RGB))  # Convert BGR->RGB for plt\n    plt.title(f\"LIME Heatmap (Pos for '{predicted_class_name}')\")\n    plt.axis('off')\n    show_heatmap = True\n    \nexcept KeyError:\n    logger.warning(f\"Predicted class index {predicted_class_idx} not found in LIME explanation weights.\")\n    plt.subplot(1, 3, 2)\n    plt.imshow(target_image_numpy)\n    plt.title(\"Original Image (LIME Heatmap Failed)\")\n    plt.axis('off')\n    show_heatmap = False\nexcept Exception as e:\n    logger.error(f\"Error generating LIME heatmap: {e}\", exc_info=True)\n    plt.subplot(1, 3, 2)\n    plt.imshow(target_image_numpy)\n    plt.title(\"Original Image (LIME Heatmap Error)\")\n    plt.axis('off')\n    show_heatmap = False\n\n# --- 3. Show Original Image for Reference ---\nplt.subplot(1, 3, 3)\nplt.imshow(target_image_numpy)  # Assumes target_image_numpy is RGB or Grayscale\nplt.title(f\"Original Image - Predicted: {predicted_class_name} ({output_prob:.4f})\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# --- BONUS: Alternative Visualization with Both Positive and Negative Features ---\n# If the above visualization fails or you want an additional view\ntry:\n    plt.figure(figsize=(12, 4))\n    \n    # Plot 1: Original image\n    plt.subplot(1, 3, 1)\n    plt.imshow(target_image_numpy)\n    plt.title(f\"Original\\nPrediction: {predicted_class_name}\")\n    plt.axis('off')\n    \n    # Plot 2: Positive features (supporting the prediction)\n    temp_pos, mask_pos = explanation_lime.get_image_and_mask(\n        predicted_class_idx,\n        positive_only=True,\n        num_features=5,\n        hide_rest=False\n    )\n    temp_pos_norm = normalize_for_display(temp_pos)\n    plt.subplot(1, 3, 2)\n    plt.imshow(mark_boundaries(temp_pos_norm, mask_pos, color=(0,1,0)))\n    plt.title(f\"Supporting Features\\n(Green)\")\n    plt.axis('off')\n    \n    # Plot 3: Negative features (contradicting the prediction)\n    try:\n        temp_neg, mask_neg = explanation_lime.get_image_and_mask(\n            predicted_class_idx,\n            positive_only=False,\n            negative_only=True, \n            num_features=5,\n            hide_rest=False\n        )\n        temp_neg_norm = normalize_for_display(temp_neg)\n        plt.subplot(1, 3, 3)\n        plt.imshow(mark_boundaries(temp_neg_norm, mask_neg, color=(1,0,0)))\n        plt.title(f\"Contradicting Features\\n(Red)\")\n        plt.axis('off')\n    except:\n        # If negative features extraction fails, show original again\n        plt.subplot(1, 3, 3)\n        plt.imshow(target_image_numpy)\n        plt.title(\"No Contradicting Features Found\")\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \nexcept Exception as fallback_e:\n    logger.warning(f\"Alternative visualization also failed: {fallback_e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:53:43.448850Z","iopub.execute_input":"2025-05-01T19:53:43.449209Z","iopub.status.idle":"2025-05-01T19:53:44.167857Z","shell.execute_reply.started":"2025-05-01T19:53:43.449186Z","shell.execute_reply":"2025-05-01T19:53:44.166112Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### --- SHAP Explanation ---","metadata":{}},{"cell_type":"code","source":"# --- 7. SHAP Explanation ---\nlogger.info(\"--- Starting SHAP Explanation ---\")\nstart_shap_time = time.time()\n\n# --- Prepare Background Data for SHAP ---\nbackground_files = []\nfor f in os.listdir(BACKGROUND_DATA_DIR):\n    file_path = os.path.join(BACKGROUND_DATA_DIR, f)\n    # Check if it's a file and has a valid image extension\n    if os.path.isfile(file_path) and f.lower().endswith(('.png', '.jpg', '.jpeg')):\n        background_files.append(file_path)\n                    \nif len(background_files) > N_SHAP_BACKGROUND_SAMPLES:\n    background_files = random.sample(background_files, N_SHAP_BACKGROUND_SAMPLES)\nelif not background_files:\n    logger.error(f\"No background image files found in {BACKGROUND_DATA_DIR}. Cannot run SHAP.\")\n    exit()\n\nlogger.info(f\"Loading {len(background_files)} background images for SHAP...\")\nbackground_tensors = []\nfor f in tqdm(background_files, desc=\"Loading Background\"):\n    img = load_image(f, transform_type='tensor')\n    if img is not None:\n        background_tensors.append(img)\n\nif not background_tensors:\n     logger.error(f\"Failed to load any background images. Cannot run SHAP.\")\n     exit()\n\nbackground_data_tensor = torch.stack(background_tensors).to(DEVICE)\nlogger.info(f\"Background data shape: {background_data_tensor.shape}\")\n\n# --- Prepare Target Image Tensor for SHAP ---\n# Needs batch dimension: (1, C, H, W)\nshap_input_tensor = target_image_tensor.unsqueeze(0).to(DEVICE)\n\n# --- Create SHAP Explainer ---\n# GradientExplainer is usually efficient for PyTorch models\n# Pass the model and the background data\nexplainer_shap = shap.GradientExplainer(model, background_data_tensor)\n\n# --- Calculate SHAP values ---\n# This can be computationally expensive\nlogger.info(\"Calculating SHAP values (this may take some time)...\")\nshap_values = explainer_shap.shap_values(shap_input_tensor)\nlogger.info(\"SHAP values calculated.\")\n\nshap_duration = time.time() - start_shap_time\nlogger.info(f\"SHAP explanation generated in {shap_duration:.2f} seconds.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:53:44.169423Z","iopub.execute_input":"2025-05-01T19:53:44.169779Z","iopub.status.idle":"2025-05-01T19:54:14.740622Z","shell.execute_reply.started":"2025-05-01T19:53:44.169757Z","shell.execute_reply":"2025-05-01T19:54:14.739434Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### --- SHAP Visualization ---","metadata":{}},{"cell_type":"code","source":"logger.info(\"--- Visualizing SHAP Results ---\")\n\n# --- Process image and SHAP values for visualization ---\n# Convert input tensor for plotting: (1, C, H, W) -> (H, W, C) and denormalize approx\nshap_input_numpy = shap_input_tensor.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n# Approximate denormalization for visualization\nshap_input_numpy = (shap_input_numpy * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\nshap_input_numpy = np.clip(shap_input_numpy, 0, 1)\n\n# Process SHAP values: Handle both list and array formats\nif isinstance(shap_values, list):\n    shap_values_processed = shap_values[0]  # Take the first element if it's a list\nelse:\n    shap_values_processed = shap_values\n\n# Reshape SHAP values for plotting: (1, C, H, W) -> (1, H, W, C)\nshap_values_plot = shap_values_processed.transpose(0, 2, 3, 1)\n\n# Log shapes for debugging\nlogger.info(f\"Shape of image for SHAP plot: {shap_input_numpy.shape}\")  # Should be (H, W, C)\nlogger.info(f\"Shape of SHAP values for plot: {shap_values_plot.shape}\")  # Should be (1, H, W, C)\n\n# --- 1. Default SHAP image_plot ---\nprint(\"\\nSHAP Default Explanation Plot:\")\ntry:\n    shap.image_plot(\n        shap_values=shap_values_plot,\n        pixel_values=np.expand_dims(shap_input_numpy, 0),\n        labels=np.array([[predicted_class_name]]),\n        show=True  # Show the plot directly\n    )\n    show_default_shap = True\nexcept Exception as e:\n    logger.error(f\"Error visualizing SHAP with default image_plot: {e}\", exc_info=True)\n    print(\"Could not generate default SHAP plot.\")\n    show_default_shap = False\n\n# --- 2. Custom SHAP Heatmap ---\nprint(\"\\nSHAP Custom Heatmap Plot:\")\ntry:\n    # shap_values_processed has shape (1, C, H, W)\n    # 1. Remove Batch dimension -> (C, H, W)\n    shap_map = shap_values_processed[0]\n    \n    # 2. Aggregate across channels\n    # Sum contributions across channels, then clip negatives\n    shap_heatmap_data = np.sum(shap_map, axis=0)  # Now (H, W)\n    \n    # 3. Keep only positive contributions (pushing towards predicted class)\n    shap_heatmap_data_pos = np.clip(shap_heatmap_data, a_min=0, a_max=None)\n    \n    # 4. Generate Overlay\n    shap_heatmap_overlay = generate_heatmap_overlay(\n        shap_input_numpy,\n        shap_heatmap_data_pos,  # Use positive contributions map\n        colormap='hot',  # 'hot', 'Reds', 'jet' are good choices\n        alpha=0.6\n    )\n    \n    # 5. Plot side by side comparison\n    plt.figure(figsize=(10, 5))\n    \n    # Heatmap overlay\n    plt.subplot(1, 2, 1)\n    plt.imshow(cv2.cvtColor(shap_heatmap_overlay, cv2.COLOR_BGR2RGB))  # Convert BGR->RGB for plt\n    plt.title(f\"SHAP Heatmap (Pos Contributions for '{predicted_class_name}')\")\n    plt.axis('off')\n    \n    # Original image\n    plt.subplot(1, 2, 2)\n    plt.imshow(shap_input_numpy)  # Already float [0, 1] suitable for plt\n    plt.title(\"Original Image (Approx. Denormalized)\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    plt.close()\n    show_shap = True\n    \nexcept Exception as e:\n    logger.error(f\"Error generating SHAP heatmap: {e}\", exc_info=True)\n    # Fallback plot\n    plt.figure(figsize=(6, 6))\n    plt.imshow(shap_input_numpy)\n    plt.title(\"Original Image (SHAP Heatmap Failed)\")\n    plt.axis('off')\n    plt.show()\n    shap_heatmap_overlay = shap_input_numpy\n    show_shap = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:54:14.742314Z","iopub.execute_input":"2025-05-01T19:54:14.742720Z","iopub.status.idle":"2025-05-01T19:54:15.516669Z","shell.execute_reply.started":"2025-05-01T19:54:14.742674Z","shell.execute_reply":"2025-05-01T19:54:15.515461Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## --- 6. Combine Visualizations ---","metadata":{}},{"cell_type":"code","source":"# Grad-CAM\nplt.subplot(2, 2, 1)\nplt.imshow(cv2.cvtColor(gradcam_heatmap_overlay, cv2.COLOR_BGR2RGB))\nplt.title(f\"Grad-CAM (Pos for '{predicted_class_name}')\")\nplt.axis('off')\n\n# LIME Boundary\nplt.subplot(2, 2, 2)\nif show_boundaries:\n    plt.imshow(mark_boundaries(temp_lime / 2 + 0.5, mask_lime))\n    plt.title(f\"LIME Boundaries (Pos for '{predicted_class_name}')\")\nelse:\n    plt.imshow(target_image_numpy)\n    plt.title(\"Original Image (LIME Boundaries Failed)\")\nplt.axis('off')\n\n# LIME Heatmap\nplt.subplot(2, 2, 3)\nif show_heatmap:\n    plt.imshow(cv2.cvtColor(lime_heatmap_overlay, cv2.COLOR_BGR2RGB))\n    plt.title(f\"LIME Heatmap (Pos for '{predicted_class_name}')\")\nelse:\n    plt.imshow(target_image_numpy)\n    plt.title(\"Original Image (LIME Heatmap Failed)\")\nplt.axis('off')\n\n# SHAP Heatmap\nplt.subplot(2, 2, 4)\nif show_shap:\n    plt.imshow(cv2.cvtColor(shap_heatmap_overlay, cv2.COLOR_BGR2RGB))\n    plt.title(f\"SHAP Heatmap (Pos for '{predicted_class_name}')\")\nelse:\n    plt.imshow(shap_input_numpy)\n    plt.title(\"Original Image (SHAP Heatmap Failed)\")\nplt.axis('off')\n\nplt.tight_layout()\ncombined_output_path = os.path.join(OUTPUT_DIR, f\"{os.path.basename(IMAGE_PATH).split('.')[0]}_combined.png\")\nplt.savefig(combined_output_path, bbox_inches='tight')\nplt.show()\nplt.close()\n\nlogger.info(f\"Combined visualization saved to {combined_output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:54:15.517755Z","iopub.execute_input":"2025-05-01T19:54:15.518102Z","iopub.status.idle":"2025-05-01T19:54:16.249502Z","shell.execute_reply.started":"2025-05-01T19:54:15.518076Z","shell.execute_reply":"2025-05-01T19:54:16.248441Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Summary","metadata":{}},{"cell_type":"code","source":"# Log a summary of the execution.\nlogger.info(\"--- Explainability Analysis Summary ---\")\nlogger.info(f\"Model: {MODEL_NAME}\")\nlogger.info(f\"Image: {IMAGE_PATH}\")\nlogger.info(f\"Predicted Class: {predicted_class_name} (Probability: {output_prob:.4f})\")\nlogger.info(f\"Grad-CAM Duration: {gradcam_duration:.2f} seconds\")\nlogger.info(f\"LIME Duration: {lime_duration:.2f} seconds\")\nlogger.info(f\"SHAP Duration: {shap_duration:.2f} seconds\")\nlogger.info(f\"Output Directory: {OUTPUT_DIR}\")\n\nlogger.info(\"--- XAI Script Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:54:16.251136Z","iopub.execute_input":"2025-05-01T19:54:16.251545Z","iopub.status.idle":"2025-05-01T19:54:16.269956Z","shell.execute_reply.started":"2025-05-01T19:54:16.251509Z","shell.execute_reply":"2025-05-01T19:54:16.268370Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}