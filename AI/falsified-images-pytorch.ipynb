{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3151739,"sourceType":"datasetVersion","datasetId":1915180},{"sourceId":11446602,"sourceType":"datasetVersion","datasetId":7171216}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Falsified Image Detection using PyTorch","metadata":{}},{"cell_type":"markdown","source":"This notebook implements a pipeline for detecting falsified images (e.g., forged signatures) using various pre-trained deep learning models with PyTorch. The process includes:\n\n1.  **Environment Setup**: Installs necessary packages and configures the environment (Kaggle, Colab, or local) for API access and path management.\n2.  **Data Preparation**:\n    *   Downloads and sets up the handwritten signature verification dataset.\n    *   Splits the dataset into training, validation, and test sets.\n    *   Cleans the dataset by removing any corrupted or invalid images.\n3.  **Data Loading**: Defines PyTorch `Dataset` and `DataLoader` instances with image augmentations using Albumentations for efficient data handling.\n4.  **Model Definition**:\n    *   Lists several pre-trained models (e.g., EfficientNet, ConvNeXt, ViT) to be experimented with.\n    *   Implements a `CustomModel` class that allows using these pre-trained backbones with a custom classification head and supports fine-tuning by unfreezing specified layers.\n5.  **Training and Evaluation**:\n    *   Defines functions for model training (`train_model`) and evaluation (`evaluate_model`). The training function includes features like early stopping, checkpointing, and integration with Optuna for hyperparameter pruning.\n    *   Implements an `objective` function for Optuna to perform hyperparameter optimization (HPO) for learning rate, dense layer units, dropout, optimizer type, batch size, and the number of unfrozen layers.\n6.  **Experimentation Loop**:\n    *   Iterates through the selected models.\n    *   For each model:\n        *   Performs HPO using Optuna.\n        *   Trains a final model using the best hyperparameters found.\n        *   Evaluates the final model on the test set.\n        *   Saves training history, test results, and model checkpoints.\n7.  **Results and Checkpoint Management**:\n    *   Prints a summary of the test results for all models.\n    *   Visualizes training history (loss/accuracy curves) and confusion matrices.\n    *   Provides functionality to package model checkpoints, metrics, and plots into a zip file for download or transfer to cloud storage (e.g., Google Drive).\n\nThe goal is to identify the best performing model and hyperparameter configuration for the task of falsified image detection.","metadata":{}},{"cell_type":"markdown","source":"## Project body :","metadata":{"_uuid":"466f1526-1b25-4f6a-ae88-21da1a4c09d3","_cell_guid":"dc9cf959-180d-4cf2-acf7-217f1fd7403c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### 0. Code configuration","metadata":{"_uuid":"6d24eccf-9dd9-4577-ac43-567e3fa70d8c","_cell_guid":"ce3a9819-794a-4160-9735-0943c6bba5b4","trusted":true,"collapsed":false,"id":"y_9yCiPbWeJz","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell installs the necessary Python packages required for the notebook. It uses `pip install` with the `--quiet` flag to suppress verbose output during installation. The packages include:\n-   `torch`, `torchvision`, `torchaudio`: Core PyTorch libraries for deep learning, computer vision, and audio processing.\n-   `optuna`: A hyperparameter optimization framework.\n-   `kaggle`: The Kaggle API for interacting with Kaggle datasets and competitions.\n-   `transformers`: Hugging Face Transformers library for pre-trained models, particularly Vision Transformers (ViT).\n-   `matplotlib`, `seaborn`, `scikit-learn`: Libraries for plotting, statistical visualization, and machine learning utilities (metrics, model selection).\n-   `pillow`: Python Imaging Library (PIL) fork for image manipulation.\n-   `timm`: PyTorch Image Models library, providing a wide range of pre-trained computer vision models.\n-   `albumentations`: A library for fast and flexible image augmentations.\n\nConfirmation messages are printed before and after the installations.","metadata":{}},{"cell_type":"code","source":"print(\"--- Installing necessary packages ---\")\n!pip install torch torchvision torchaudio --quiet\n!pip install optuna --quiet\n!pip install kaggle --quiet\n!pip install transformers --quiet\n!pip install matplotlib seaborn scikit-learn --quiet\n!pip install pillow --quiet\n!pip install --upgrade timm --quiet\n!pip install -U albumentations --quiet\nprint(\"--- Installations complete ---\")","metadata":{"_uuid":"f80f1a92-486e-486b-a5c3-14cab2d7ff69","_cell_guid":"d1583bf4-52a7-4d16-9e73-807a225ee031","trusted":true,"id":"TSAd1-T4Win4","outputId":"bf06bcbf-fe65-42fb-b714-c2af5323b777","executionInfo":{"status":"ok","timestamp":1742976934223,"user_tz":-60,"elapsed":149804,"user":{"displayName":"Khalil Krifi","userId":"13701701284808241864"}},"execution":{"iopub.status.busy":"2025-05-07T22:49:41.005429Z","iopub.execute_input":"2025-05-07T22:49:41.005781Z","iopub.status.idle":"2025-05-07T22:50:18.386437Z","shell.execute_reply.started":"2025-05-07T22:49:41.005751Z","shell.execute_reply":"2025-05-07T22:50:18.385422Z"}},"outputs":[{"name":"stdout","text":"--- Installing necessary packages ---\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h--- Installations complete ---\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### 1. Import necessary libraries :","metadata":{"_uuid":"c91c1ed2-0e7a-4a2b-862a-ec3ca5221018","_cell_guid":"aa4f8439-456f-4d92-9df2-1f064a878359","trusted":true,"collapsed":false,"id":"7QlC3qJ1q2ly","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell imports all the Python libraries and modules that will be used throughout the notebook. This includes:\n-   PyTorch modules (`torch`, `torch.nn`, `torchvision`, `DataLoader`, `Dataset`).\n-   Hugging Face Transformers (`ViTModel`, `AutoImageProcessor`).\n-   Standard libraries (`os`, `shutil`, `numpy`, `subprocess`, `time`, `datetime`, `copy`, `pathlib`, `logging`, `sqlite3`, `glob`, `sys`, `json`, `zipfile`, `random`).\n-   Data science and visualization libraries (`sklearn` for metrics and model selection, `PIL` for image handling, `matplotlib` and `seaborn` for plotting).\n-   `timm` for pre-trained image models.\n-   `optuna` for hyperparameter optimization.\n-   `albumentations` for image augmentations.\n-   `IPython.display` for creating download links in Kaggle/Colab.\n-   `tqdm` for progress bars.\n\nIt also sets up basic logging configuration to display informational messages with timestamps.","metadata":{}},{"cell_type":"code","source":"print(\"--- Importing libraries ---\")\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport os\nimport shutil\nimport numpy as np\nimport optuna\nimport subprocess\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport timm\nimport glob\nimport sys\nimport json \nimport zipfile\nimport random\nimport cv2\nimport albumentations as A\nimport copy\nimport logging\nimport sqlite3 \nfrom torchvision import transforms, datasets, models as torchvision_models\nfrom transformers import ViTModel, AutoImageProcessor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\nfrom PIL import Image, UnidentifiedImageError\nfrom torch.utils.data import DataLoader, Dataset\nfrom datetime import timedelta\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom albumentations.pytorch import ToTensorV2\nfrom IPython.display import FileLink\n\nlogger = logging.getLogger(__name__)\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(message)s', \n    datefmt='%m/%d %H:%M:%S', \n    handlers=[\n        logging.StreamHandler(sys.stdout)\n    ],\n    force=True  \n)\nprint(\"--- Library imports complete ---\")","metadata":{"_uuid":"10238629-baae-40b1-87fe-c781c71f8cb2","_cell_guid":"01aa77e9-0b0e-4271-9bd3-6cfd91951e9f","trusted":true,"id":"Czy5QkV1KaT1","executionInfo":{"status":"ok","timestamp":1742976983867,"user_tz":-60,"elapsed":49589,"user":{"displayName":"Khalil Krifi","userId":"13701701284808241864"}},"outputId":"9e3bb35f-7f2d-4eaa-d2ad-8311a48e0083","execution":{"iopub.status.busy":"2025-05-07T22:50:18.387704Z","iopub.execute_input":"2025-05-07T22:50:18.387980Z","iopub.status.idle":"2025-05-07T22:50:47.988957Z","shell.execute_reply.started":"2025-05-07T22:50:18.387956Z","shell.execute_reply":"2025-05-07T22:50:47.988067Z"}},"outputs":[{"name":"stdout","text":"--- Importing libraries ---\n--- Library imports complete ---\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Authentication :","metadata":{"_uuid":"a1c36d7d-af70-4fd7-a127-62b1066569d6","_cell_guid":"5bea2608-e9b8-4166-b44c-f25581daebc0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell defines a function `configure_kaggle_credentials` to detect the execution environment (Kaggle, Colab, or unknown) and configure Kaggle API credentials accordingly.\n-   If running in a **Kaggle environment**, it sets `ENVIRONMENT` to \"kaggle\".\n-   If running in a **Colab environment**, it sets `ENVIRONMENT` to \"colab\", attempts to mount Google Drive, and creates a directory in Google Drive for storing checkpoints.\n-   If the environment is **unknown**, it sets `ENVIRONMENT` to \"unknown\" and warns the user to configure credentials manually.\nThe function is then called, and the detected `ENVIRONMENT` is printed.","metadata":{}},{"cell_type":"code","source":"def configure_kaggle_credentials():\n    global ENVIRONMENT \n    if os.path.exists(\"/kaggle/working/\"):\n        ENVIRONMENT = \"kaggle\"\n        logger.info(\"Detected Kaggle environment.\")\n    elif os.path.exists(\"/content\"):\n        ENVIRONMENT = \"colab\"\n        logger.info(\"Detected Colab environment.\")\n\n        try:\n            from google.colab import files \n            from google.colab import drive\n            if not os.path.exists('/content/drive'):\n                logger.info(\"Mounting Google Drive...\")\n                drive.mount('/content/drive')\n            \n            drive_dir = os.path.join(\n                '/content/drive/Shareddrives/PCD/checkpoints/SignatureVerification'\n            )\n            os.makedirs(drive_dir, exist_ok=True)\n            \n        except ImportError as e:\n            logger.error(f\"Colab libraries unavailable: {str(e)}\")\n            print(\"Colab environment detected but required libraries are missing.\")\n            return False\n    else:\n        ENVIRONMENT = \"unknown\"\n        logger.warning(\"Unknown environment detected.\")\n        print(\"Unknown environment detected. Please configure Kaggle credentials manually.\")\n        return False\n\nsuccess = configure_kaggle_credentials()\nENVIRONMENT","metadata":{"_uuid":"f48c9cd5-a98e-46fa-9948-2141ff640ddc","_cell_guid":"7060ceb4-d024-4796-9f14-c689d69f34c7","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T22:50:47.990379Z","iopub.execute_input":"2025-05-07T22:50:47.990996Z","iopub.status.idle":"2025-05-07T22:50:48.002001Z","shell.execute_reply.started":"2025-05-07T22:50:47.990968Z","shell.execute_reply":"2025-05-07T22:50:48.001034Z"}},"outputs":[{"name":"stdout","text":"05/07 22:50:47 - Detected Kaggle environment.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'kaggle'"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"### 2. Checkpoints and Tunner paths :","metadata":{"_uuid":"bf1ecafa-ec16-4cdc-994f-7c4d74f1a843","_cell_guid":"18bb5d5c-acf6-4cd2-8da4-989f39a8a3cd","trusted":true,"collapsed":false,"id":"oblWbo9QZGKh","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell sets up the paths for storing model checkpoints and the Optuna database file based on the `ENVIRONMENT` detected in the previous cell.\n-   `CHECKPOINT_BASE_DIR`: The base directory where model checkpoints will be saved.\n-   `OPTUNA_DB_PATH`: The SQLite database path for Optuna to store hyperparameter tuning trial results.\nIt ensures that the checkpoint base directory exists by creating it if necessary and logs the configured paths.","metadata":{}},{"cell_type":"code","source":"if ENVIRONMENT == 'kaggle':\n    CHECKPOINT_BASE_DIR = \"/kaggle/working/checkpoints/SignatureVerification\"\n    OPTUNA_DB_PATH = \"sqlite:////kaggle/working/optuna_signature_verification.db\"\nelif ENVIRONMENT == 'colab':\n    CHECKPOINT_BASE_DIR = \"/content/checkpoints/SignatureVerification\"\n    OPTUNA_DB_PATH = \"sqlite:////content/optuna_signature_verification.db\"\nelse:\n    CHECKPOINT_BASE_DIR = \"./checkpoints/SignatureVerification\"\n    OPTUNA_DB_PATH = \"sqlite:///optuna_signature_verification.db\"\nos.makedirs(CHECKPOINT_BASE_DIR, exist_ok=True)\nlogger.info(f\"Checkpoint directory set to: {CHECKPOINT_BASE_DIR}\")\nlogger.info(f\"Optuna DB path set to: {OPTUNA_DB_PATH}\")","metadata":{"_uuid":"e60724dd-3acd-40ef-85b3-53f5c89767a7","_cell_guid":"39a851ba-df58-409f-a957-9a6969bbfa35","trusted":true,"id":"D-4e-tXLZF0-","executionInfo":{"status":"ok","timestamp":1742977014055,"user_tz":-60,"elapsed":30193,"user":{"displayName":"Khalil Krifi","userId":"13701701284808241864"}},"outputId":"730291ce-8b37-4beb-ad88-d234fd5b2014","execution":{"iopub.status.busy":"2025-05-07T22:50:48.003407Z","iopub.execute_input":"2025-05-07T22:50:48.003719Z","iopub.status.idle":"2025-05-07T22:50:48.040446Z","shell.execute_reply.started":"2025-05-07T22:50:48.003696Z","shell.execute_reply":"2025-05-07T22:50:48.039637Z"}},"outputs":[{"name":"stdout","text":"05/07 22:50:48 - Checkpoint directory set to: /kaggle/working/checkpoints/SignatureVerification\n05/07 22:50:48 - Optuna DB path set to: sqlite:////kaggle/working/optuna_signature_verification.db\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### 3. DataSet Setup  :","metadata":{"_uuid":"8fd471d4-caf5-40a1-98e8-af56b49279a2","_cell_guid":"3c7fd243-b5bb-4fb1-8c01-7e5df40f6349","trusted":true,"collapsed":false,"id":"o28S9tf7qEiz","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell configures the primary dataset (`handwritten-signature-verification` by `tienen`) and handles its download and extraction based on the execution environment.\n-   It defines the dataset name, owner, and zip file name.\n-   **Kaggle Environment**: It sets `BASE_DIR` to the expected path of the dataset within the Kaggle input directory. It raises an error if the dataset is not found.\n-   **Colab Environment**: It sets `UNZIP_TARGET_DIR` and `BASE_DIR`. If the data is not found at the expected path, it downloads the dataset using the Kaggle API (`kaggle datasets download`) and unzips it using the `unzip` command. It includes error handling for download and unzipping processes and removes the zip file after successful extraction.\n-   **Unknown/Local Environment**: It assumes the dataset is already present locally at a predefined path and sets `BASE_DIR` accordingly. It raises an error if the dataset is not found.\nThe cell logs the actions taken and the final `BASE_DIR` used for accessing the dataset.","metadata":{}},{"cell_type":"code","source":"DATASET_NAME = \"handwritten-signature-verification\"\nDATASET_OWNER = \"tienen\"\nZIP_FILE = f\"{DATASET_NAME}.zip\"\n\n# Detect environment and set up dataset\nprint(\"\\n--- Setting and Dataset ---\")\nif ENVIRONMENT == 'kaggle':\n    BASE_DIR = \"/kaggle/input/handwritten-signature-verification/data/data\"\n    if not os.path.exists(BASE_DIR):\n        raise FileNotFoundError(\n            f\"Kaggle dataset not found at expected path: {BASE_DIR}. \"\n            \"Please verify the dataset is attached to the notebook.\"\n        )\n    logger.info(f\"Using Kaggle dataset path: {BASE_DIR}\")\nelif ENVIRONMENT == 'colab':\n    UNZIP_TARGET_DIR = \"/content/data\"\n    CORRECT_DATA_PATH = os.path.join(UNZIP_TARGET_DIR, \"data/data\")\n    BASE_DIR = CORRECT_DATA_PATH\n\n    if not os.path.exists(CORRECT_DATA_PATH):\n        logger.info(f\"Data not found at {CORRECT_DATA_PATH}. Downloading and unzipping...\")\n        if os.path.exists(UNZIP_TARGET_DIR):\n            logger.warning(f\"Removing existing directory {UNZIP_TARGET_DIR} before unzipping.\")\n            shutil.rmtree(UNZIP_TARGET_DIR)\n        os.makedirs(UNZIP_TARGET_DIR, exist_ok=True)\n\n        # Download dataset\n        try:\n            cmd = [\"kaggle\", \"datasets\", \"download\", \"-d\", f\"{DATASET_OWNER}/{DATASET_NAME}\", \"-p\", \"/content\"]\n            subprocess.run(cmd, check=True, text=True, capture_output=True)\n            logger.info(\"Dataset download complete.\")\n        except subprocess.CalledProcessError as e:\n            logger.error(f\"Error during dataset download: {e.stderr}\")\n            print(f\"Download failed: {e.stderr}\")\n            exit(1)\n        except Exception as e:\n            logger.error(f\"Error during dataset download: {e}\")\n            print(f\"Download failed: {e}\")\n            exit(1)\n\n        # Unzip dataset\n        zip_path = os.path.join(\"/content\", ZIP_FILE)\n        logger.info(f\"Unzipping {ZIP_FILE} to {UNZIP_TARGET_DIR}...\")\n        try:\n            result = subprocess.run(\n                [\"unzip\", \"-q\", zip_path, \"-d\", UNZIP_TARGET_DIR],\n                capture_output=True,\n                text=True,\n                check=True\n            )\n            logger.info(\"Unzipping successful.\")\n            os.remove(zip_path)\n            logger.info(\"Deleted zip file.\")\n\n            if not os.path.exists(CORRECT_DATA_PATH):\n                logger.error(\n                    f\"Unzipped successfully, but expected data path {CORRECT_DATA_PATH} not found. \"\n                    \"Check dataset structure.\"\n                )\n                print(f\"Error: Expected path {CORRECT_DATA_PATH} not found after unzipping.\")\n                exit(1)\n            else:\n                logger.info(f\"Verified data path exists: {CORRECT_DATA_PATH}\")\n        except subprocess.CalledProcessError as e:\n            logger.error(f\"Unzipping failed: {e.stderr}\")\n            print(f\"Unzipping failed: {e.stderr}\")\n            exit(1)\n        except Exception as e:\n            logger.error(f\"Error during unzipping or verification: {e}\")\n            print(f\"Error during unzipping: {e}\")\n            exit(1)\n    else:\n        logger.info(f\"Dataset already exists at {CORRECT_DATA_PATH}.\")\nelse:\n    logger.warning(\"Unknown environment detected. Assuming local dataset.\")\n    BASE_DIR = \"./data/handwritten-signature-verification/data/data\"\n    if not os.path.exists(BASE_DIR):\n        raise FileNotFoundError(\n            f\"Dataset not found at expected local path: {BASE_DIR}. \"\n            \"Please place the dataset manually or adjust BASE_DIR.\"\n        )\n    logger.info(f\"Using local dataset path: {BASE_DIR}\")","metadata":{"_uuid":"f4407eb1-8596-4268-89f8-ee8242924eb7","_cell_guid":"15b63df5-362e-45be-81e1-ef2a81e79eee","trusted":true,"id":"3pJ_2P2OJQfs","outputId":"b8a041c7-74b3-401e-ff95-13aa59a7b0bf","executionInfo":{"status":"error","timestamp":1742977080150,"user_tz":-60,"elapsed":66117,"user":{"displayName":"Khalil Krifi","userId":"13701701284808241864"}},"execution":{"iopub.status.busy":"2025-05-07T22:50:48.041518Z","iopub.execute_input":"2025-05-07T22:50:48.041846Z","iopub.status.idle":"2025-05-07T22:50:48.058979Z","shell.execute_reply.started":"2025-05-07T22:50:48.041814Z","shell.execute_reply":"2025-05-07T22:50:48.058113Z"}},"outputs":[{"name":"stdout","text":"\n--- Setting and Dataset ---\n05/07 22:50:48 - Using Kaggle dataset path: /kaggle/input/handwritten-signature-verification/data/data\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### 4. GPU check and Configuration","metadata":{"_uuid":"52c495ed-b88c-4d57-baca-50d10176ebf3","_cell_guid":"b75c315d-7230-4988-a114-50a7d6667f7e","trusted":true,"collapsed":false,"id":"mBkkoqDTzT0T","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell checks for the availability of a CUDA-enabled GPU and configures PyTorch to use it.\n-   It prints the current PyTorch version.\n-   If a GPU is available (`torch.cuda.is_available()` is true):\n    -   It sets the `device` to `torch.device(\"cuda\")`.\n    -   It logs the name of the GPU being used.\n    -   It optionally clears the CUDA cache using `torch.cuda.empty_cache()`.\n-   If no GPU is available, it sets the `device` to `torch.device(\"cpu\")` and logs that the CPU will be used.","metadata":{}},{"cell_type":"code","source":"print(\"\\n--- Checking GPU Configuration ---\")\nprint(\"PyTorch version:\", torch.__version__)\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    logging.info(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n    torch.cuda.empty_cache()\nelse:\n    device = torch.device(\"cpu\")\n    logging.info(\"Using CPU\")","metadata":{"_uuid":"345d5766-da56-4a0c-8a62-96c05cdb69fd","_cell_guid":"81f7132f-d63e-4123-9988-3ae8a6cbef85","trusted":true,"id":"Sd_g1PahzWzq","executionInfo":{"status":"aborted","timestamp":1742977080097,"user_tz":-60,"elapsed":299060,"user":{"displayName":"Khalil Krifi","userId":"13701701284808241864"}},"execution":{"iopub.status.busy":"2025-05-07T22:50:48.059957Z","iopub.execute_input":"2025-05-07T22:50:48.060291Z","iopub.status.idle":"2025-05-07T22:50:48.198118Z","shell.execute_reply.started":"2025-05-07T22:50:48.060267Z","shell.execute_reply":"2025-05-07T22:50:48.197236Z"}},"outputs":[{"name":"stdout","text":"\n--- Checking GPU Configuration ---\nPyTorch version: 2.5.1+cu121\n05/07 22:50:48 - Using GPU: Tesla T4\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### 5. Define constants :","metadata":{"_uuid":"c1f0d6eb-108c-4cab-be03-cb15724cada3","_cell_guid":"d7d52489-d123-4b16-bc24-cec41103630a","trusted":true,"collapsed":false,"id":"E9NjaZY4rWNd","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell defines several constants and paths used throughout the notebook:\n-   `model_img_sizes`: A dictionary mapping model names (e.g., \"EfficientNetV2-S\", \"ConvNeXt_Base\") to their required input image dimensions (height, width). This is crucial for preprocessing images correctly for each model.\n-   `SPLIT_BASE_DIR`: The base directory where the split dataset (train, validation, test) will be stored. This path is determined based on the `ENVIRONMENT` (Colab or Kaggle/local).\n-   `TRAIN_DIR`, `VAL_DIR`, `TEST_DIR`: Full paths to the training, validation, and test subdirectories within `SPLIT_BASE_DIR`.\n-   `OPTUNA_DB_PATH`: Re-defines the Optuna database path if it wasn't set globally, ensuring its availability. It also ensures the directory for the Optuna database exists.\n-   It creates the `TRAIN_DIR`, `VAL_DIR`, and `TEST_DIR` directories if they don't already exist.\n-   Finally, it verifies that `BASE_DIR` (from Cell 3, the raw dataset path) is correctly set, raising an error if not.","metadata":{}},{"cell_type":"code","source":"print(\"\\n--- Defining Constants ---\")\nmodel_img_sizes = {\n    \"EfficientNetV2-S\": (384, 384), \n    \"ConvNeXt_Base\": (224, 224),    \n    \"DeiT_Base\": (224, 224),        \n    \"BEiT_Large\": (224, 224),       \n    \"EfficientNet_B7\": (600, 600),  \n    \"ResNetRS50\": (224, 224),       \n    \"InceptionV3\": (299, 299),      \n    \"Xception\": (299, 299),         \n    \"ViT_Base\": (224, 224),         \n    \"MobileNetV3_Large\": (224, 224) \n}\n\nif 'SPLIT_BASE_DIR' not in globals(): \n    if  ENVIRONMENT == 'colab':\n         SPLIT_BASE_DIR = '/content/split_dataset'\n    else: \n         SPLIT_BASE_DIR = '/kaggle/working/split_dataset' \n    logging.info(f\"SPLIT_BASE_DIR defined  as: {SPLIT_BASE_DIR}\")\n\n\nTRAIN_DIR = os.path.join(SPLIT_BASE_DIR, 'train')\nVAL_DIR = os.path.join(SPLIT_BASE_DIR, 'val')\nTEST_DIR = os.path.join(SPLIT_BASE_DIR, 'test')\n\nif 'OPTUNA_DB_PATH' not in globals(): \n    if  ENVIRONMENT == 'colab':\n        OPTUNA_DB_PATH = \"sqlite:////content/optuna_signature_verification.db\"\n    else: \n        OPTUNA_DB_PATH = \"sqlite:////kaggle/working/optuna_signature_verification.db\"\n    logging.info(f\"OPTUNA_DB_PATH defined  as: {OPTUNA_DB_PATH}\")\n\ndb_dir = os.path.dirname(OPTUNA_DB_PATH.replace(\"sqlite:///\", \"\"))\nif db_dir and not os.path.exists(db_dir):\n    os.makedirs(db_dir, exist_ok=True)\n\n\n# Create split directories\nos.makedirs(TRAIN_DIR, exist_ok=True)\nos.makedirs(VAL_DIR, exist_ok=True)\nos.makedirs(TEST_DIR, exist_ok=True)\n\nif 'BASE_DIR' not in globals():\n    raise NameError(\"CRITICAL ERROR: BASE_DIR was not set correctly in Section 4!\")\nelse:\n    logging.info(f\"Verified BASE_DIR : {BASE_DIR}\")","metadata":{"_uuid":"f7b54cf7-7714-4d29-86ad-7079b4515263","_cell_guid":"e5287501-29a9-4d7e-a471-cb9c7fc2e025","trusted":true,"id":"MWWjV2QMK9SL","executionInfo":{"status":"aborted","timestamp":1742977080100,"user_tz":-60,"elapsed":299057,"user":{"displayName":"Khalil Krifi","userId":"13701701284808241864"}},"execution":{"iopub.status.busy":"2025-05-07T22:50:48.199284Z","iopub.execute_input":"2025-05-07T22:50:48.199700Z","iopub.status.idle":"2025-05-07T22:50:48.214298Z","shell.execute_reply.started":"2025-05-07T22:50:48.199660Z","shell.execute_reply":"2025-05-07T22:50:48.213438Z"}},"outputs":[{"name":"stdout","text":"\n--- Defining Constants ---\n05/07 22:50:48 - SPLIT_BASE_DIR defined  as: /kaggle/working/split_dataset\n05/07 22:50:48 - Verified BASE_DIR : /kaggle/input/handwritten-signature-verification/data/data\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### 6. Function to split the dataset into train, validation, and test sets :","metadata":{"_uuid":"e15a34eb-708a-450a-9431-56d31fedc56c","_cell_guid":"6707615b-8eaf-49d1-afb0-e0b45d614eaa","trusted":true,"collapsed":false,"id":"_vjARejIrozB","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell defines and then calls the `split_dataset` function.\nThe function `split_dataset` takes the raw dataset directory (`base_dir`), target directories for train, validation, and test sets, and splitting ratios as input.\nIts key functionalities are:\n-   **Ratio Validation**: Ensures that `train_ratio`, `val_ratio`, and `test_ratio` are valid (between 0 and 1, and sum to 1).\n-   **Skip if Already Split**: Checks if the split directories (`train_dir`, `val_dir`, `test_dir`) already exist and contain data. If so, and `force_resplit` is `False`, it skips the splitting process and prints a summary of the existing split.\n-   **Force Resplit**: If `force_resplit` is `True`, it clears any existing split directories before proceeding.\n-   **Directory Creation**: Creates subdirectories for each class ('real', 'forged') within the train, validation, and test directories.\n-   **Data Splitting**:\n    -   Iterates through each class ('real', 'forged') in the `base_dir`.\n    -   Collects all image file paths for the current class.\n    -   Shuffles the image list for randomness (with a fixed seed for reproducibility).\n    -   Splits the shuffled list into train, validation, and test sets based on the provided ratios.\n    -   Copies the image files to their respective target directories (e.g., `train_dir/real`, `val_dir/forged`). It handles potential filename collisions by appending a counter if a file with the same name already exists in the destination.\n-   **Summary Print**: After splitting, it prints a summary of the number of 'real' and 'forged' images in each split (train, validation, test).\n\nThe function is then called with `BASE_DIR` (raw data), `TRAIN_DIR`, `VAL_DIR`, `TEST_DIR`, and `force_resplit=False` (meaning it will use existing splits if available, otherwise it will perform the split).","metadata":{}},{"cell_type":"code","source":"def split_dataset(base_dir, train_dir, val_dir, test_dir, train_ratio=0.7, val_ratio=0.15, force_resplit=False):\n    logging.info(\"--- Starting Dataset Split ---\")\n    test_ratio = 1.0 - train_ratio - val_ratio\n    if not (0 < train_ratio < 1 and 0 < val_ratio < 1 and 0 < test_ratio < 1 and abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6):\n        raise ValueError(\"Ratios must be between 0 and 1, and sum to 1.\")\n\n    # Check if splitting is needed\n    if not force_resplit and \\\n       os.path.exists(os.path.join(train_dir, 'real')) and len(os.listdir(os.path.join(train_dir, 'real'))) > 0 and \\\n       os.path.exists(os.path.join(val_dir, 'real')) and len(os.listdir(os.path.join(val_dir, 'real'))) > 0 and \\\n       os.path.exists(os.path.join(test_dir, 'real')) and len(os.listdir(os.path.join(test_dir, 'real'))) > 0:\n        logging.info(\"Split dataset already exists. Skipping split.\")\n        # Print summary of existing split\n        print(\"Existing Dataset split summary:\")\n        for split, dir_path in [('Train', train_dir), ('Validation', val_dir), ('Test', test_dir)]:\n            try:\n                real_count = len(os.listdir(os.path.join(dir_path, 'real'))) if os.path.exists(os.path.join(dir_path, 'real')) else 0\n                forged_count = len(os.listdir(os.path.join(dir_path, 'forged'))) if os.path.exists(os.path.join(dir_path, 'forged')) else 0\n                print(f\"  {split}: {real_count} real, {forged_count} forged images\")\n            except FileNotFoundError:\n                 print(f\"  {split}: Class directory not found (real or forged).\")\n        return\n\n    logging.info(f\"Splitting data from {base_dir} with ratios: Train={train_ratio}, Val={val_ratio}, Test={test_ratio}\")\n\n    # Clear existing split directories if force_resplit is True\n    if force_resplit:\n        logging.warning(\"Force resplit enabled. Clearing existing split directories.\")\n        for split_dir in [train_dir, val_dir, test_dir]:\n            if os.path.exists(split_dir):\n                shutil.rmtree(split_dir)\n\n    # Create directories for train, val, and test with class subfolders\n    for split in ['train', 'val', 'test']:\n        for class_name in ['real', 'forged']:\n            os.makedirs(os.path.join(SPLIT_BASE_DIR, split, class_name), exist_ok=True)\n\n    # Split data for each class\n    all_copied_files = set() \n    for class_name in ['real', 'forged']:\n        class_path = os.path.join(base_dir, class_name)\n        if not os.path.isdir(class_path):\n            logging.error(f\"Class directory not found: {class_path}. Check BASE_DIR and dataset structure.\")\n            continue \n\n        images = []\n        for root, _, files in os.walk(class_path):\n            for file in files:\n                if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):\n                    full_path = os.path.join(root, file)\n                    if full_path not in all_copied_files: \n                        images.append(full_path)\n\n\n        logging.info(f\"Found {len(images)} images in class '{class_name}'.\")\n        if not images:\n            logging.warning(f\"No images found for class '{class_name}'. Check directory structure and file types.\")\n            continue\n\n        # Ensure reproducibility\n        np.random.seed(42)\n        np.random.shuffle(images)\n\n        # Calculate split indices\n        n_total = len(images)\n        n_train = int(n_total * train_ratio)\n        n_val = int(n_total * val_ratio)\n        # n_test = n_total - n_train - n_val # Remainder goes to test\n\n        train_images = images[:n_train]\n        val_images = images[n_train : n_train + n_val]\n        test_images = images[n_train + n_val :]\n\n        # Function to copy files\n        def copy_files(file_list, target_dir, class_n):\n            target_class_dir = os.path.join(target_dir, class_n)\n            copied_count = 0\n            for img_path in file_list:\n                try:\n                    # Use a simpler destination name (original filename)\n                    dest_name = os.path.basename(img_path)\n                    dest_path = os.path.join(target_class_dir, dest_name)\n                    # Handle potential filename collisions (though less likely with shuffle)\n                    counter = 1\n                    while os.path.exists(dest_path):\n                        name, ext = os.path.splitext(dest_name)\n                        dest_path = os.path.join(target_class_dir, f\"{name}_{counter}{ext}\")\n                        counter += 1\n\n                    shutil.copy2(img_path, dest_path) \n                    all_copied_files.add(img_path) \n                    copied_count += 1\n                except Exception as e:\n                    logging.error(f\"Failed to copy {img_path} to {dest_path}: {e}\")\n            return copied_count\n\n        # Copy images\n        logging.info(f\"Copying {class_name} images...\")\n        train_copied = copy_files(train_images, train_dir, class_name)\n        val_copied = copy_files(val_images, val_dir, class_name)\n        test_copied = copy_files(test_images, test_dir, class_name)\n        logging.info(f\"  Copied to Train: {train_copied}, Val: {val_copied}, Test: {test_copied}\")\n\n\n    # Print final split summary\n    logging.info(\"--- Dataset split completed ---\")\n    print(\"Final Dataset split summary:\")\n    for split, dir_path in [('Train', train_dir), ('Validation', val_dir), ('Test', test_dir)]:\n        try:\n            real_count = len(os.listdir(os.path.join(dir_path, 'real'))) if os.path.exists(os.path.join(dir_path, 'real')) else 0\n            forged_count = len(os.listdir(os.path.join(dir_path, 'forged'))) if os.path.exists(os.path.join(dir_path, 'forged')) else 0\n            print(f\"  {split}: {real_count} real, {forged_count} forged images (Total: {real_count + forged_count})\")\n        except FileNotFoundError:\n             print(f\"  {split}: Class directory not found (real or forged).\")\n\n\n# Split the dataset (force_resplit=False by default)\nsplit_dataset(BASE_DIR, TRAIN_DIR, VAL_DIR, TEST_DIR, force_resplit=False)","metadata":{"_uuid":"e6a10e50-3d68-4a9e-9d5a-153fbeb294dc","_cell_guid":"b62bedcb-c55c-4dc5-8612-0d38d9e3e47a","trusted":true,"id":"DnGwxvCUUo9w","executionInfo":{"status":"aborted","timestamp":1742977080187,"user_tz":-60,"elapsed":26,"user":{"displayName":"Khalil Krifi","userId":"13701701284808241864"}},"execution":{"iopub.status.busy":"2025-05-07T22:50:48.218087Z","iopub.execute_input":"2025-05-07T22:50:48.218499Z","iopub.status.idle":"2025-05-07T22:51:46.482643Z","shell.execute_reply.started":"2025-05-07T22:50:48.218441Z","shell.execute_reply":"2025-05-07T22:51:46.481843Z"}},"outputs":[{"name":"stdout","text":"05/07 22:50:48 - --- Starting Dataset Split ---\n05/07 22:50:48 - Splitting data from /kaggle/input/handwritten-signature-verification/data/data with ratios: Train=0.7, Val=0.15, Test=0.15000000000000005\n05/07 22:50:54 - Found 3188 images in class 'real'.\n05/07 22:50:54 - Copying real images...\n05/07 22:51:22 -   Copied to Train: 2231, Val: 478, Test: 479\n05/07 22:51:24 - Found 2984 images in class 'forged'.\n05/07 22:51:24 - Copying forged images...\n05/07 22:51:46 -   Copied to Train: 2088, Val: 447, Test: 449\n05/07 22:51:46 - --- Dataset split completed ---\nFinal Dataset split summary:\n  Train: 2231 real, 2088 forged images (Total: 4319)\n  Validation: 478 real, 447 forged images (Total: 925)\n  Test: 479 real, 449 forged images (Total: 928)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### 7. Removing damaged images:","metadata":{"_uuid":"499387a4-a856-4396-a5cc-6b6846395255","_cell_guid":"21f5c366-187c-44d8-9619-c3994bd8e501","trusted":true,"collapsed":false,"id":"IXNTI-Zlr-fL","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell defines and uses the `check_and_remove_images_quiet` function to ensure data integrity by removing corrupted or unreadable images from the dataset splits.\nThe function `check_and_remove_images_quiet`:\n-   Takes a directory path (e.g., `TRAIN_DIR`) as input.\n-   Iterates through 'real' and 'forged' subdirectories within the given directory.\n-   Collects all image files (common extensions like .png, .jpg, etc.).\n-   For each image file:\n    -   Tries to open and verify it using `PIL.Image.open()` and `img.verify()`, then `img.load()` to ensure it's fully readable.\n    -   If any `UnidentifiedImageError`, `IOError`, `SyntaxError`, `FileNotFoundError`, or other exception occurs during this process, the image is considered invalid.\n    -   Invalid images are removed from the filesystem using `os.remove()`.\n-   It displays a progress bar showing the number of files checked and removed.\n-   Prints a summary of checked and removed files for the processed directory.\n\nThe cell then iterates through `TRAIN_DIR`, `VAL_DIR`, and `TEST_DIR`, calling `check_and_remove_images_quiet` on each to clean all dataset splits. A final summary of the total number of files removed across all sets is printed.","metadata":{}},{"cell_type":"code","source":"def check_and_remove_images_quiet(directory):\n    print(f\"--- Checking for invalid images in '{directory}' ---\")\n\n    removed_files = 0\n    checked_files = 0\n    files_to_check = []\n\n    # 1. Collect files first for accurate progress\n    for class_name in ['real', 'forged']:\n        class_dir = os.path.join(directory, class_name)\n        if not os.path.isdir(class_dir):\n            continue\n        try:\n            for filename in os.listdir(class_dir):\n                file_path = os.path.join(class_dir, filename)\n                if os.path.isfile(file_path) and filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff')):\n                     files_to_check.append(file_path)\n        except OSError as e:\n             print(f\"\\nError listing directory '{class_dir}': {e}\") \n\n    total_files = len(files_to_check)\n    if total_files == 0:\n        print(\"No image files found to check in this directory.\")\n        return 0\n\n    # 2. Process files and update progress bar\n    for idx, file_path in enumerate(files_to_check):\n        checked_files += 1\n        is_valid = True\n        try:\n            with Image.open(file_path) as img:\n                img.verify()\n            with Image.open(file_path) as img:\n                img.load()\n        except (UnidentifiedImageError, IOError, SyntaxError):\n            is_valid = False\n        except FileNotFoundError:\n\n            is_valid = False\n            checked_files -=1\n        except Exception:\n            is_valid = False\n\n        if not is_valid:\n            try:\n                if os.path.exists(file_path):\n                    os.remove(file_path)\n                    removed_files += 1\n            except OSError:\n                pass\n            except Exception:\n                pass\n\n\n        progress_message = f\"Checked: {idx+1}/{total_files} files | Removed: {removed_files}\"\n        print(f\"\\r{progress_message:<80}\", end=\"\") \n        sys.stdout.flush()\n\n    # 3. Print final summary\n    print() \n    print(f\"Finished check for '{directory}'. Checked {checked_files} files. Removed {removed_files} invalid files.\")\n    return removed_files\n\n\n\nprint(\"\\n--- Starting Image Validity Check ---\")\ntotal_removed_quiet = 0\nfor data_dir in [TRAIN_DIR, VAL_DIR, TEST_DIR]:\n    if os.path.exists(data_dir):\n        total_removed_quiet += check_and_remove_images_quiet(data_dir)\n    else:\n        print(f\"Directory '{data_dir}' not found. Skipping validity check.\")\n\nprint(f\"--- Image validity check complete. Total files removed across all sets: {total_removed_quiet} ---\")","metadata":{"_uuid":"8621546f-a44e-40a4-b7e7-acb3f992f46a","_cell_guid":"0956256c-c896-4042-aaad-9c073f3ab019","trusted":true,"id":"UjfirjYtUuEt","executionInfo":{"status":"aborted","timestamp":1742977080198,"user_tz":-60,"elapsed":1,"user":{"displayName":"Khalil Krifi","userId":"13701701284808241864"}},"execution":{"iopub.status.busy":"2025-05-07T22:51:46.483957Z","iopub.execute_input":"2025-05-07T22:51:46.484201Z","iopub.status.idle":"2025-05-07T22:53:07.182686Z","shell.execute_reply.started":"2025-05-07T22:51:46.484180Z","shell.execute_reply":"2025-05-07T22:53:07.181742Z"}},"outputs":[{"name":"stdout","text":"\n--- Starting Image Validity Check ---\n--- Checking for invalid images in '/kaggle/working/split_dataset/train' ---\nChecked: 4319/4319 files | Removed: 1                                           \nFinished check for '/kaggle/working/split_dataset/train'. Checked 4319 files. Removed 1 invalid files.\n--- Checking for invalid images in '/kaggle/working/split_dataset/val' ---\nChecked: 925/925 files | Removed: 0                                             \nFinished check for '/kaggle/working/split_dataset/val'. Checked 925 files. Removed 0 invalid files.\n--- Checking for invalid images in '/kaggle/working/split_dataset/test' ---\nChecked: 928/928 files | Removed: 0                                             \nFinished check for '/kaggle/working/split_dataset/test'. Checked 928 files. Removed 0 invalid files.\n--- Image validity check complete. Total files removed across all sets: 1 ---\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### 8. Data Loaders","metadata":{"_uuid":"c4d19ecf-425e-4656-9e23-1feb8ff08c69","_cell_guid":"a04e6308-8ea7-453b-8322-17ee27f49045","trusted":true,"collapsed":false,"id":"mvQBd7EY1sjq","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell defines the necessary components for loading and transforming image data for PyTorch models.\nIt consists of two main parts:\n\n1.  **`AlbumentationsDataset` Class**:\n    *   A custom PyTorch `Dataset` class that wraps a list of image paths and their corresponding labels.\n    *   It takes an Albumentations `transform` pipeline as an argument.\n    *   In `__getitem__`, it loads an image from a path, converts it to RGB, converts it to a NumPy array, and then applies the specified Albumentations transformations.\n    *   Includes error handling for `FileNotFoundError`, `UnidentifiedImageError`, and other exceptions during image loading, returning a placeholder tensor and a special label (-1) in case of an error.\n\n2.  **`get_data_loaders` Function**:\n    *   This function creates and returns PyTorch `DataLoader` instances for the training, validation, and test sets.\n    *   **Transforms**: It defines separate Albumentations transform pipelines for training (`train_transform`) and validation/testing (`val_test_transform`).\n        *   `train_transform` includes resizing, various augmentations like rotation, flips, affine transformations, perspective changes, color jitter, Gaussian blur, normalization, and conversion to a PyTorch tensor.\n        *   `val_test_transform` includes only resizing, normalization, and tensor conversion.\n    *   **Data Collection**: It uses a helper function `collect_paths_labels` to gather image file paths and their corresponding labels (0 for 'real', 1 for 'forged') from the `TRAIN_DIR`, `VAL_DIR`, and `TEST_DIR`.\n    *   **Dataset Instantiation**: It creates instances of `AlbumentationsDataset` for train, validation, and test sets using the collected paths, labels, and the appropriate transform pipelines.\n    *   **Logging**: It logs dataset sizes and class distributions. It also checks for empty datasets and raises a `ValueError` if any split is empty.\n    *   **DataLoader Creation**: It creates `DataLoader` objects for each dataset, specifying batch size, shuffle behavior (True for training, False for validation/test), `num_workers` (set to 0 in this version for compatibility, but can be increased for parallel data loading), and `pin_memory`.\n    *   It logs a success message upon completion.","metadata":{}},{"cell_type":"code","source":"class AlbumentationsDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None, img_size=(224, 224)):\n        if len(image_paths) != len(labels):\n            raise ValueError(f\"Number of image paths ({len(image_paths)}) does not match number of labels ({len(labels)}).\")\n\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n        self.img_size = img_size \n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        label = self.labels[idx]\n\n        try:\n            image = Image.open(image_path).convert('RGB')\n            image_np = np.array(image)\n\n            if self.transform:\n                augmented = self.transform(image=image_np)\n                image_tensor = augmented['image']\n            else:\n                basic_transform = A.Compose([\n                    A.Resize(self.img_size[0], self.img_size[1]),\n                    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                    ToTensorV2()\n                ])\n                image_tensor = basic_transform(image=image_np)['image']\n\n            return image_tensor, torch.tensor(label, dtype=torch.long)\n\n        except FileNotFoundError:\n            logging.error(f\"File not found in __getitem__: {image_path}. Check dataset integrity.\")\n            placeholder_img = torch.zeros((3, self.img_size[0], self.img_size[1]), dtype=torch.float32)\n            return placeholder_img, torch.tensor(-1, dtype=torch.long)\n        except UnidentifiedImageError:\n             logging.error(f\"Corrupted or unidentified image format in __getitem__: {image_path}.\")\n             placeholder_img = torch.zeros((3, self.img_size[0], self.img_size[1]), dtype=torch.float32)\n             return placeholder_img, torch.tensor(-1, dtype=torch.long)\n        except Exception as e:\n            logging.error(f\"Error processing image {image_path} in __getitem__: {e}\", exc_info=True)\n            placeholder_img = torch.zeros((3, self.img_size[0], self.img_size[1]), dtype=torch.float32)\n            return placeholder_img, torch.tensor(-1, dtype=torch.long)\n\ndef get_data_loaders(model_name, batch_size, img_size, num_workers=2): \n    logging.info(f\"Creating DataLoaders for {model_name} (Img Size: {img_size}, Batch Size: {batch_size}, Num Workers: {num_workers})...\") \n    imagenet_mean = [0.485, 0.456, 0.406]\n    imagenet_std = [0.229, 0.224, 0.225]\n    \n\n    rotation_p = 0.3; h_flip_p = 0.5; v_flip_p = 0.0; affine_p = 0.3\n    color_jitter_p = 0.5; blur_p = 0.3; perspective_p = 0.1\n\n    train_transform = A.Compose([\n        A.Resize(img_size[0], img_size[1]),\n        A.Rotate(limit=20, p=rotation_p, border_mode=cv2.BORDER_REFLECT_101),\n        A.HorizontalFlip(p=h_flip_p),\n        A.VerticalFlip(p=v_flip_p),\n        A.Affine(scale=(0.85, 1.15), translate_percent=(-0.1, 0.1), shear=(-10, 10), p=affine_p, keep_ratio=True),\n        A.Perspective(scale=(0.03, 0.08), p=perspective_p, keep_size=True),\n        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=color_jitter_p),\n        A.GaussianBlur(blur_limit=(3, 7), sigma_limit=(0.1, 1.5), p=blur_p),\n        A.Normalize(mean=imagenet_mean, std=imagenet_std),\n        ToTensorV2(),\n    ])\n\n    val_test_transform = A.Compose([\n        A.Resize(img_size[0], img_size[1]),\n        A.Normalize(mean=imagenet_mean, std=imagenet_std),\n        ToTensorV2(),\n    ])\n\n    # --- Collect Image Paths and Labels---\n    def collect_paths_labels(split_dir):\n        paths = []\n        labels = []\n        for class_idx, class_name in enumerate(['real', 'forged']):\n            class_path = os.path.join(split_dir, class_name)\n            if not os.path.isdir(class_path):\n                logging.warning(f\"Class directory not found: {class_path}. Skipping.\")\n                continue\n            try:\n                for filename in os.listdir(class_path):\n                    if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff')):\n                        image_path = os.path.join(class_path, filename)\n                        if os.path.isfile(image_path):\n                            paths.append(image_path)\n                            labels.append(class_idx)\n            except OSError as e:\n                logging.error(f\"Error listing files in {class_path}: {e}\")\n        return paths, labels\n\n    train_image_paths, train_labels = collect_paths_labels(TRAIN_DIR)\n    val_image_paths, val_labels = collect_paths_labels(VAL_DIR)\n    test_image_paths, test_labels = collect_paths_labels(TEST_DIR)\n\n    # --- Create Datasets---\n    train_dataset = AlbumentationsDataset(train_image_paths, train_labels, transform=train_transform, img_size=img_size)\n    val_dataset = AlbumentationsDataset(val_image_paths, val_labels, transform=val_test_transform, img_size=img_size)\n    test_dataset = AlbumentationsDataset(test_image_paths, test_labels, transform=val_test_transform, img_size=img_size)\n\n    # --- Log Dataset Sizes and Class Distribution ---\n    base_train_size = len(train_dataset); base_val_size = len(val_dataset); base_test_size = len(test_dataset)\n    train_real_count = sum(1 for label in train_labels if label == 0); train_forged_count = len(train_labels) - train_real_count\n    val_real_count = sum(1 for label in val_labels if label == 0); val_forged_count = len(val_labels) - val_real_count\n    test_real_count = sum(1 for label in test_labels if label == 0); test_forged_count = len(test_labels) - test_real_count\n\n    logging.info(f\"Dataset sizes: Train={base_train_size}, Validation={base_val_size}, Test={base_test_size}\")\n    logging.info(f\"  Train class distribution: Real={train_real_count}, Forged={train_forged_count}\")\n\n\n    # --- Check for Empty Datasets---\n    if base_train_size == 0 or base_val_size == 0 or base_test_size == 0:\n        error_msg = \"One or more datasets are empty!\" \n        logging.error(error_msg)\n        raise ValueError(error_msg)\n\n\n    # --- Create DataLoaders---\n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True,\n        num_workers=0, \n        pin_memory=torch.cuda.is_available(), drop_last=False\n    )\n    val_loader = DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False,\n        num_workers=0, \n        pin_memory=torch.cuda.is_available()\n    )\n    test_loader = DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False,\n        num_workers=0, \n        pin_memory=torch.cuda.is_available()\n    )\n\n    logging.info(f\"DataLoaders created successfully for {model_name}.\")\n    return train_loader, val_loader, test_loader","metadata":{"_uuid":"07884ce4-2bd9-467a-a491-66e30eeecf15","_cell_guid":"a261aaf3-9341-4fa1-a125-ce04109b8a9d","trusted":true,"id":"KiBkXXzNp8PB","executionInfo":{"status":"aborted","timestamp":1742977080207,"user_tz":-60,"elapsed":1,"user":{"displayName":"Khalil Krifi","userId":"13701701284808241864"}},"execution":{"iopub.status.busy":"2025-05-07T22:53:07.183613Z","iopub.execute_input":"2025-05-07T22:53:07.183880Z","iopub.status.idle":"2025-05-07T22:53:07.203700Z","shell.execute_reply.started":"2025-05-07T22:53:07.183843Z","shell.execute_reply":"2025-05-07T22:53:07.202771Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### 9. Define pre-trained models :","metadata":{"_uuid":"1f58c92e-7be6-4558-b708-e4cbaea21656","_cell_guid":"a5786f80-584b-478a-831e-cafa0c44b4fd","collapsed":false,"id":"t2D7i9-Hs1TT","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell defines a list named `models_to_try`. This list contains the names of various pre-trained model architectures that the notebook will experiment with for the image classification task.\n\nAfter defining the list, it iterates through `models_to_try` to verify that the required input image sizes for each of these models are defined in the `model_img_sizes` dictionary (which was defined in Cell 5). If a model's image size is not found, it raises a `KeyError`, ensuring that the notebook has all necessary configuration before proceeding to model building.","metadata":{}},{"cell_type":"code","source":"print(\"\\n--- Defining Base Models ---\")\nmodels_to_try = [\n     # \"ConvNeXt_Base\",        \n     # \"EfficientNetV2-S\",     \n     # \"DeiT_Base\",           \n     # \"BEiT_Base\",          \n     # \"ResNetRS50\",          \n     # \"Xception\",             \n     # \"MobileNetV3_Large\",   \n     \"ViT_Base\",             \n     # \"EfficientNet_B7\",      \n     # \"InceptionV3\",         \n]\n\n# Verify image sizes are defined for all selected models\nfor model_name in models_to_try:\n    if model_name not in model_img_sizes:\n        raise KeyError(f\"Image size for model '{model_name}' not defined in 'model_img_sizes'.\")","metadata":{"_uuid":"a844a6b4-3384-431c-9447-d2a41a30b720","_cell_guid":"be11dc87-bad2-4cac-95cd-5eea87460818","trusted":true,"id":"OJ9rVlB1tCMU","executionInfo":{"status":"aborted","timestamp":1742977080438,"user_tz":-60,"elapsed":30,"user":{"displayName":"Khalil Krifi","userId":"13701701284808241864"}},"execution":{"iopub.status.busy":"2025-05-07T22:53:07.204759Z","iopub.execute_input":"2025-05-07T22:53:07.205107Z","iopub.status.idle":"2025-05-07T22:53:07.222931Z","shell.execute_reply.started":"2025-05-07T22:53:07.205057Z","shell.execute_reply":"2025-05-07T22:53:07.221845Z"}},"outputs":[{"name":"stdout","text":"\n--- Defining Base Models ---\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### 10. Function to build and train a model :","metadata":{"_uuid":"4002211e-3fc2-4fa9-b4d4-ca2fbcb6bac2","_cell_guid":"29573e96-20b9-41af-9f30-a301d4aac55b","trusted":true,"collapsed":false,"id":"6ER39_vGtOvU","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell defines the `CustomModel` class, a PyTorch `nn.Module`, which serves as a flexible wrapper for creating image classification models using various pre-trained backbones. The key enhancement in this version is more specific logic for unfreezing layers in different architectures.\n\nKey features of the `CustomModel` class:\n-   **Initialization (`__init__`)**:\n    -   Takes `model_name`, `dense_units` (for the classifier head), `dropout` probability, `pretrained` flag, and `unfreeze_layers` (number of layer groups/blocks/stages to unfreeze in the base model) as arguments.\n    -   **Base Model Loading**:\n        -   If `model_name` is \"ViT_Base\", it loads a Vision Transformer from Hugging Face Transformers (`google/vit-base-patch16-224`). It configures it not to add a pooling layer, as the CLS token's embedding will be used directly.\n        -   For other model names, it uses `timm.create_model` to load the corresponding pre-trained model. A mapping (`timm_model_name_map`) is used to get the specific `timm` model identifier. `num_classes=0` is used to get the feature extractor part of the `timm` models.\n    -   **Feature Dimension Verification**: It performs a test forward pass with a dummy input tensor to determine the actual number of output features from the base model. This helps ensure the classifier head is correctly sized.\n    -   **Parameter Freezing/Unfreezing**:\n        -   Initially, all parameters in the loaded base model are frozen (`param.requires_grad = False`).\n        -   If `unfreeze_layers` is greater than 0, it proceeds to unfreeze the specified number of layers/blocks from the end of the model. The unfreezing logic is now model-architecture-specific:\n            -   **ViT_Base**: Unfreezes the last `unfreeze_layers` transformer encoder layers (`self.base_model.encoder.layer`).\n            -   **EfficientNet models**: Attempts to unfreeze the last `unfreeze_layers` from `self.base_model.blocks`. If `blocks` attribute is not found, it falls back to `_generic_unfreeze`.\n            -   **ConvNeXt models**: Attempts to unfreeze the last `unfreeze_layers` from `self.base_model.stages`. If `stages` attribute is not found, it falls back to `_generic_unfreeze`.\n            -   **ResNet models**: Identifies main layer groups (e.g., `layer1`, `layer2`), sorts them, and unfreezes the last `unfreeze_layers` of these groups. Additionally, it unfreezes parameters in any top-level modules named 'norm', 'bn', or 'head'.\n            -   **Other models**: Uses the `_generic_unfreeze` helper method.\n        -   Logs the number of initially frozen parameters and the final counts of trainable and frozen parameters after the unfreezing process.\n    -   **`_generic_unfreeze` Helper Method**: This private method is used as a fallback. It collects all named modules that have parameters, sorts them (attempting to approximate depth), and unfreezes the parameters of the last `unfreeze_layers` modules in this list.\n    -   **Classifier Head**: Defines a sequential classifier head consisting of a `Linear` layer, `ReLU` activation, `BatchNorm1d`, `Dropout`, and a final `Linear` layer with a single output unit (for binary classification logits).\n-   **Forward Pass (`forward`)**:\n    -   Takes an input tensor `x`.\n    -   Passes `x` through the base model to get features.\n        -   For ViT, it extracts the embedding of the `[CLS]` token (`last_hidden_state[:, 0]`).\n        -   For `timm` models, it uses the direct output (pooled features).\n    -   Passes the extracted features through the custom classifier head.\n    -   Returns the output logits.","metadata":{}},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, model_name, dense_units, dropout, pretrained=True, unfreeze_layers=0):\n        super(CustomModel, self).__init__()\n        self.model_name = model_name\n        self.base_model = None\n        reported_features = 0  \n        \n        logging.info(f\"Initializing CustomModel: Base='{model_name}', DenseUnits={dense_units}, Dropout={dropout:.2f}, Unfreeze={unfreeze_layers}\")\n        \n        try:\n            # --- Load Base Model ---\n            if model_name == \"ViT_Base\":\n                hf_model_name = 'google/vit-base-patch16-224'\n                self.base_model = ViTModel.from_pretrained(\n                    hf_model_name,\n                    add_pooling_layer=False,  \n                    ignore_mismatched_sizes=True\n                )\n                reported_features = self.base_model.config.hidden_size\n                logging.info(f\"Loaded '{hf_model_name}' from HuggingFace Transformers. Reported features: {reported_features}\")\n            else:\n                timm_model_name_map = {\n                    \"ConvNeXt_Base\": \"convnext_base.fb_in22k_ft_in1k\", \n                    \"EfficientNetV2-S\": \"tf_efficientnetv2_s.in21k_ft_in1k\",\n                    \"DeiT_Base\": \"deit_base_patch16_224.fb_in1k\",\n                    \"BEiT_Base\": \"beit_base_patch16_224\",\n                    \"EfficientNet_B7\": \"tf_efficientnet_b7.ns_jft_in1k\", \n                    \"ResNetRS50\": \"resnetrs50.tf_in1k\",\n                    \"InceptionV3\": \"inception_v3.tf_in1k\",\n                    \"Xception\": \"xception.tf_in1k\",\n                    \"MobileNetV3_Large\": \"mobilenetv3_large_100.miil_in21k_ft_in1k\",\n                }\n                if model_name not in timm_model_name_map:\n                    raise ValueError(f\"Model name '{model_name}' not found in timm map or not supported.\")\n\n                timm_name = timm_model_name_map[model_name]\n                self.base_model = timm.create_model(timm_name, pretrained=pretrained, num_classes=0)\n                reported_features = self.base_model.num_features\n                logging.info(f\"Loaded '{timm_name}' from timm. Reported features: {reported_features}\")\n            \n            # --- Verify Actual Feature Dimensions with Test Forward Pass ---\n            self.base_model.eval()\n            with torch.no_grad():\n                try:\n                    dummy_input = torch.zeros(1, 3, 224, 224)\n                    if model_name == \"ViT_Base\":\n                        actual_features = self.base_model(dummy_input).last_hidden_state[:, 0]\n                    else:\n                        actual_features = self.base_model(dummy_input)\n                    \n                    actual_num_features = actual_features.shape[1]\n                    \n                    if actual_num_features != reported_features:\n                        logging.warning(f\"Model '{model_name}' reports {reported_features} features but outputs {actual_num_features} features. Using actual dimension.\")\n                        num_features = actual_num_features\n                    else:\n                        num_features = reported_features\n                        logging.info(f\"Verified that model '{model_name}' outputs {num_features} features as expected.\")\n                        \n                except Exception as e:\n                    logging.warning(f\"Exception during feature verification: {e}. Using reported feature count {reported_features}.\")\n                    num_features = reported_features\n\n            # --- Parameter Freezing/Unfreezing ---\n            if unfreeze_layers < 0:\n                logging.warning(f\"unfreeze_layers cannot be negative ({unfreeze_layers}). Setting to 0 (fully frozen).\")\n                unfreeze_layers = 0\n\n            for param in self.base_model.parameters():\n                param.requires_grad = False\n            frozen_params = sum(p.numel() for p in self.base_model.parameters() if not p.requires_grad)\n            logging.info(f\"Initially froze {frozen_params:,} parameters in the base model.\")\n\n            if unfreeze_layers > 0:\n                layers_unfrozen_count = 0\n                params_unfrozen_count = 0\n                \n                if model_name == \"ViT_Base\":\n                    num_layers = len(self.base_model.encoder.layer)\n                    layers_to_unfreeze = min(unfreeze_layers, num_layers)\n                    \n                    for i in range(num_layers - layers_to_unfreeze, num_layers):\n                        for param in self.base_model.encoder.layer[i].parameters():\n                            param.requires_grad = True\n                            params_unfrozen_count += param.numel()\n                        \n                        layers_unfrozen_count += 1\n                        logging.info(f\"Unfroze ViT encoder layer {i}\")\n                \n                elif \"EfficientNet\" in model_name:\n                    if hasattr(self.base_model, 'blocks'):\n                        blocks = self.base_model.blocks\n                        num_blocks = len(blocks)\n                        blocks_to_unfreeze = min(unfreeze_layers, num_blocks)\n                        \n                        for i in range(num_blocks - blocks_to_unfreeze, num_blocks):\n                            for param in blocks[i].parameters():\n                                param.requires_grad = True\n                                params_unfrozen_count += param.numel()\n                            \n                            layers_unfrozen_count += 1\n                            logging.info(f\"Unfroze EfficientNet block {i}\")\n                    else:\n                        logging.warning(f\"Could not find blocks for {model_name}, falling back to generic unfreezing.\")\n                        self._generic_unfreeze(unfreeze_layers, layers_unfrozen_count, params_unfrozen_count)\n                \n                elif \"ConvNeXt\" in model_name:\n                    if hasattr(self.base_model, 'stages'):\n                        stages = self.base_model.stages\n                        num_stages = len(stages)\n                        stages_to_unfreeze = min(unfreeze_layers, num_stages)\n                        \n                        for i in range(num_stages - stages_to_unfreeze, num_stages):\n                            for param in stages[i].parameters():\n                                param.requires_grad = True\n                                params_unfrozen_count += param.numel()\n                            \n                            layers_unfrozen_count += 1\n                            logging.info(f\"Unfroze ConvNeXt stage {i}\")\n                    else:\n                        logging.warning(f\"Could not find stages for {model_name}, falling back to generic unfreezing.\")\n                        self._generic_unfreeze(unfreeze_layers, layers_unfrozen_count, params_unfrozen_count)\n                \n                elif \"ResNet\" in model_name:\n                    layer_groups = []\n                    for name, child in self.base_model.named_children():\n                        if 'layer' in name and isinstance(child, nn.Module):\n                            layer_groups.append((name, child))\n                    \n                    layer_groups.sort(key=lambda x: x[0])  \n                    groups_to_unfreeze = min(unfreeze_layers, len(layer_groups))\n                    \n                    for i in range(len(layer_groups) - groups_to_unfreeze, len(layer_groups)):\n                        name, layer = layer_groups[i]\n                        for param in layer.parameters():\n                            param.requires_grad = True\n                            params_unfrozen_count += param.numel()\n                        \n                        layers_unfrozen_count += 1\n                        logging.info(f\"Unfroze ResNet {name}\")\n                    \n                    for name, module in self.base_model.named_children():\n                        if any(x in name.lower() for x in ['norm', 'bn', 'head']):\n                            for param in module.parameters():\n                                param.requires_grad = True\n                                params_unfrozen_count += param.numel()\n                            logging.info(f\"Unfroze {name}\")\n                \n                else:\n                    self._generic_unfreeze(unfreeze_layers, layers_unfrozen_count, params_unfrozen_count)\n                \n                final_frozen_params = sum(p.numel() for p in self.base_model.parameters() if not p.requires_grad)\n                final_unfrozen_params = sum(p.numel() for p in self.base_model.parameters() if p.requires_grad)\n                logging.info(f\"Base model state after unfreeze: {final_unfrozen_params:,} trainable params, {final_frozen_params:,} frozen params.\")\n            else:\n                logging.info(f\"Base model remains fully frozen as requested (unfreeze_layers=0).\")\n\n            # --- Define Classifier Head ---\n            self.classifier = nn.Sequential(\n                nn.Linear(num_features, dense_units),\n                nn.ReLU(),\n                nn.BatchNorm1d(dense_units),\n                nn.Dropout(dropout),\n                nn.Linear(dense_units, 1),  \n            )\n            classifier_params = sum(p.numel() for p in self.classifier.parameters())\n            logging.info(f\"Classifier head created with {classifier_params:,} trainable parameters.\")\n            \n        except Exception as e:\n            logging.error(f\"Error initializing model '{model_name}': {e}\", exc_info=True)\n            raise\n\n    def _generic_unfreeze(self, unfreeze_layers, layers_unfrozen_count, params_unfrozen_count):\n        module_list = []\n        for name, module in self.base_model.named_modules():\n            if name and any(has_params(module) for has_params in [lambda m: any(p.requires_grad is not None for p in m.parameters(recurse=False))]):\n                module_list.append((name, module))\n        module_list.sort(key=lambda x: len(x[0].split('.')))\n        modules_to_unfreeze = module_list[-unfreeze_layers:] if unfreeze_layers < len(module_list) else module_list\n        \n        for name, module in modules_to_unfreeze:\n            has_params = False\n            for param in module.parameters(recurse=False):\n                param.requires_grad = True\n                params_unfrozen_count += param.numel()\n                has_params = True\n            \n            if has_params:\n                layers_unfrozen_count += 1\n                logging.info(f\"Unfroze layer: {name}\")\n\n    def forward(self, x):\n        if self.model_name == \"ViT_Base\":\n            features = self.base_model(x).last_hidden_state[:, 0]  \n        else:\n            features = self.base_model(x)\n        if torch.is_grad_enabled() and random.random() < 0.01:  \n            logging.debug(f\"Model '{self.model_name}' feature shape: {features.shape}\")\n        output = self.classifier(features)\n        return output","metadata":{"_uuid":"8849c469-c4a0-4cc5-8838-7e892a428a96","_cell_guid":"ef81117b-d033-4887-a2a1-4be57a630bd8","trusted":true,"id":"SDPui-CFtU8t","executionInfo":{"status":"aborted","timestamp":1742977080448,"user_tz":-60,"elapsed":1,"user":{"displayName":"Khalil Krifi","userId":"13701701284808241864"}},"execution":{"iopub.status.busy":"2025-05-07T22:53:07.224047Z","iopub.execute_input":"2025-05-07T22:53:07.224320Z","iopub.status.idle":"2025-05-07T22:53:07.253668Z","shell.execute_reply.started":"2025-05-07T22:53:07.224297Z","shell.execute_reply":"2025-05-07T22:53:07.252424Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### 11. Evaluate Model","metadata":{"_uuid":"b71e491a-dbe7-4ffa-b352-fb4968d31eb3","_cell_guid":"c11e070b-cfab-44c2-8eb9-b2c39ced3a26","trusted":true,"collapsed":false,"id":"JjmowQ_114aW","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell defines the `evaluate_model` function, which is responsible for assessing the performance of a trained model on a given dataset (typically the validation or test set).\nThe function performs the following steps:\n1.  **Set to Evaluation Mode**: Puts the `model` into evaluation mode (`model.eval()`) to disable layers like dropout and batch normalization updates.\n2.  **No Gradient Calculation**: Uses `torch.no_grad()` to ensure that gradients are not computed during evaluation, saving memory and computation.\n3.  **Iterate Through Data**: Loops through the `data_loader` (e.g., validation or test loader) batch by batch using `tqdm` for a progress bar.\n    -   Moves `inputs` and `labels` to the specified `device`. Labels are converted to `float` and unsqueezed to match the shape expected by `BCEWithLogitsLoss`.\n    -   Performs a forward pass: `outputs = model(inputs)`.\n    -   Calculates the `loss` using the provided `criterion`.\n    -   Accumulates the `total_loss`.\n    -   Converts model outputs (logits) to probabilities using `torch.sigmoid()`.\n    -   Determines predictions by thresholding probabilities at 0.5.\n    -   Stores all true `labels`, predicted `preds`, and `probs` for later metric calculation.\n4.  **Calculate Metrics**:\n    -   Calculates the average loss (`avg_loss`) over the entire dataset.\n    -   Uses `sklearn.metrics` to compute:\n        -   `accuracy_score`\n        -   `f1_score` (binary average, with `pos_label=1` and `zero_division=0` to handle cases with no positive predictions/labels gracefully)\n        -   `precision_score` (binary average, `pos_label=1`, `zero_division=0`)\n        -   `recall_score` (binary average, `pos_label=1`, `zero_division=0`)\n        -   `confusion_matrix` (with explicit labels `[0, 1]` to ensure consistent order for 'real' and 'forged' classes).\n5.  **Return Metrics**: Returns the calculated average loss, accuracy, F1-score, precision, recall, and the confusion matrix.\nIt includes a check for an empty dataset to prevent division by zero and returns zero metrics in such a case.\n","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, data_loader, criterion, device):\n\n    model.eval()  \n    total_loss = 0.0\n    all_labels = []\n    all_preds = []\n    all_probs = [] \n\n    with torch.no_grad(): \n        \n        eval_progress = tqdm(data_loader, desc=\"Evaluating\", leave=False, unit=\"batch\")\n        for inputs, labels in eval_progress:\n            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1) \n\n            outputs = model(inputs) \n            loss = criterion(outputs, labels)\n            total_loss += loss.item() * inputs.size(0) \n            probs = torch.sigmoid(outputs) \n            preds = (probs > 0.5).float() \n\n            all_labels.extend(labels.cpu().numpy().flatten())\n            all_preds.extend(preds.cpu().numpy().flatten())\n            all_probs.extend(probs.cpu().numpy().flatten())\n\n    # --- Calculate Metrics ---\n    num_samples = len(data_loader.dataset) \n    if num_samples == 0:\n        logging.warning(\"Evaluation dataset is empty. Returning zero metrics.\")\n        return 0.0, 0.0, 0.0, 0.0, 0.0, np.zeros((2, 2), dtype=int)\n    avg_loss = total_loss / num_samples\n\n    all_labels_np = np.array(all_labels)\n    all_preds_np = np.array(all_preds)\n\n    accuracy = accuracy_score(all_labels_np, all_preds_np)\n    f1 = f1_score(all_labels_np, all_preds_np, average='binary', pos_label=1, zero_division=0) \n    precision = precision_score(all_labels_np, all_preds_np, average='binary', pos_label=1, zero_division=0)\n    recall = recall_score(all_labels_np, all_preds_np, average='binary', pos_label=1, zero_division=0)\n    conf_matrix = confusion_matrix(all_labels_np, all_preds_np, labels=[0, 1]) \n\n    return avg_loss, accuracy, f1, precision, recall, conf_matrix","metadata":{"_uuid":"c2df550d-6d59-4db6-be7f-49a5195457be","_cell_guid":"609aee36-de42-414c-98e6-96edc703e887","trusted":true,"id":"K5S_9rhM12_j","executionInfo":{"status":"aborted","timestamp":1742977080456,"user_tz":-60,"elapsed":1,"user":{"displayName":"Khalil Krifi","userId":"13701701284808241864"}},"execution":{"iopub.status.busy":"2025-05-07T22:53:07.255079Z","iopub.execute_input":"2025-05-07T22:53:07.255545Z","iopub.status.idle":"2025-05-07T22:53:07.274369Z","shell.execute_reply.started":"2025-05-07T22:53:07.255508Z","shell.execute_reply":"2025-05-07T22:53:07.273183Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### 12. Hyperparameter Tuning and Training","metadata":{"_uuid":"d62be9de-9677-43e6-bb25-0d0f1c6142bb","_cell_guid":"23714d93-0309-4eb5-b67f-a39693fcd8f9","trusted":true,"collapsed":false,"id":"KxVFB__dLM1u","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell defines the `train_model` function, which orchestrates the model training process for a specified number of epochs.\nKey functionalities include:\n-   **Initialization**: Sets up variables for tracking the best validation loss, epochs without improvement (for early stopping), and the best model state. Initializes a `history` dictionary to store training and validation metrics per epoch.\n-   **Checkpoint Setup**: Defines the checkpoint directory and path for saving the best model based on validation loss.\n-   **Epoch Loop**: Iterates for the specified number of `epochs`.\n    -   **Training Phase**:\n        -   Sets the model to training mode (`model.train()`).\n        -   Iterates through the `train_loader` (with a `tqdm` progress bar).\n        -   For each batch:\n            -   Moves inputs and labels to the `device`. Handles potential placeholder labels (-1) from `AlbumentationsDataset` by skipping problematic batches or filtering them.\n            -   Performs the standard training steps: zero gradients, forward pass, loss calculation, backward pass, and optimizer step.\n            -   Accumulates training loss and calculates batch accuracy.\n        -   Calculates average training loss and accuracy for the epoch and stores them in `history`.\n    -   **Validation Phase**:\n        -   Calls the `evaluate_model` function (defined in the previous cell) to get validation metrics (loss, accuracy, F1, precision, recall).\n        -   Stores these validation metrics in `history`.\n    -   **Logging**: Logs a summary of training and validation metrics for the current epoch, along with the epoch duration.\n    -   **Early Stopping & Checkpointing**:\n        -   If the current validation loss is better than `best_val_loss`, it updates `best_val_loss`, resets `epochs_no_improve`, and saves the current model's state dictionary (`best_model_state`) to the `checkpoint_path`.\n        -   If validation loss does not improve, increments `epochs_no_improve`.\n    -   **Optuna Pruning Integration**:\n        -   If a `trial` object (from Optuna) is provided and the current epoch is beyond `hpo_warmup_steps`:\n            -   It reports the validation F1 score (`val_f1`) to the Optuna `trial`.\n            -   It checks if the `trial.should_prune()`. If true, it logs a pruning message, cleans up GPU memory, and raises `optuna.TrialPruned` to stop the trial.\n    -   **Early Stopping Check**: If `epochs_no_improve` reaches the `patience` limit, it logs an early stopping message, prints the best validation loss, and breaks the training loop.\n-   **Post-Training**:\n    -   Logs the total training duration.\n    -   Loads the `best_model_state` (if one was saved) back into the model. If no improvement was seen, it warns and uses the model state from the last epoch (and saves it).\n-   **Plotting Training History**:\n    -   Uses `matplotlib` and `seaborn` to generate plots of training/validation loss vs. epochs and training/validation accuracy & F1-score vs. epochs.\n    -   Saves these plots to the model's checkpoint directory.\n-   **Return**: Returns the trained `model` (with best weights loaded) and the `history` dictionary.","metadata":{}},{"cell_type":"code","source":"def train_model(model, model_name, train_loader, val_loader, criterion, optimizer, epochs, device, trial=None, patience=5, hpo_warmup_steps=0):\n \n    best_val_loss = float('inf')\n    epochs_no_improve = 0\n    best_model_state = None\n    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'val_f1': [], 'val_precision': [], 'val_recall': []}\n\n    # --- Checkpoint Setup ---\n    checkpoint_dir = os.path.join(CHECKPOINT_BASE_DIR, model_name)\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    checkpoint_path = os.path.join(checkpoint_dir, f\"{model_name}_best_val_loss.pth\")\n    logging.info(f\"Best model checkpoint (based on val loss) will be saved to: {checkpoint_path}\")\n\n    total_start_time = time.time()\n    logging.info(f\"--- Starting Training: {model_name} for {epochs} epochs (Patience: {patience}) ---\")\n    model.to(device) \n\n    for epoch in range(epochs):\n        epoch_start_time = time.time()\n\n        # --- Training Phase ---\n        model.train()  \n        running_loss = 0.0\n        correct_train = 0\n        total_train = 0\n\n        train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} Train\", leave=False, unit=\"batch\")\n        for inputs, labels in train_progress:\n            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1) \n\n            if torch.any(labels == -1):\n                 logging.warning(f\"Skipping batch due to error loading data (label = -1 found).\")\n                 valid_indices = (labels != -1).squeeze()\n                 if not torch.any(valid_indices): continue \n                 inputs = inputs[valid_indices]\n                 labels = labels[valid_indices]\n                 if inputs.nelement() == 0: continue\n\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            loss.backward()\n            optimizer.step()\n\n            batch_loss = loss.item()\n            running_loss += batch_loss * inputs.size(0)\n            with torch.no_grad():\n                 preds = (torch.sigmoid(outputs) > 0.5).float()\n                 total_train += labels.size(0)\n                 correct_train += (preds == labels).sum().item()\n\n            train_progress.set_postfix(batch_loss=f\"{batch_loss:.4f}\")\n\n        epoch_train_loss = running_loss / len(train_loader.sampler) \n        epoch_train_acc = correct_train / total_train if total_train > 0 else 0.0\n        history['train_loss'].append(epoch_train_loss)\n        history['train_acc'].append(epoch_train_acc)\n\n        # --- Validation Phase ---\n        val_loss, val_accuracy, val_f1, val_precision, val_recall, _ = evaluate_model(model, val_loader, criterion, device)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_accuracy)\n        history['val_f1'].append(val_f1)\n        history['val_precision'].append(val_precision)\n        history['val_recall'].append(val_recall)\n\n\n        epoch_duration = time.time() - epoch_start_time\n        logging.info(\n            f\"Epoch {epoch+1}/{epochs} | \"\n            f\"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | \"\n            f\"Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f} | Val F1: {val_f1:.4f} | \"\n            f\"Time: {timedelta(seconds=int(epoch_duration))}\"\n        )\n\n\n        # --- Early Stopping & Checkpointing (based on Validation Loss) ---\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            epochs_no_improve = 0\n            best_model_state = copy.deepcopy(model.state_dict())\n            try:\n                torch.save(best_model_state, checkpoint_path)\n                logging.info(f\"  -> Val loss improved to {val_loss:.4f}. Saved best model checkpoint.\")\n            except Exception as e:\n                 logging.error(f\"Error saving checkpoint to {checkpoint_path}: {e}\", exc_info=True)\n        else:\n            epochs_no_improve += 1\n            logging.info(f\"  (Val loss did not improve for {epochs_no_improve} epoch(s). Best: {best_val_loss:.4f})\")\n\n\n        # --- Optuna Pruning (based on Validation F1-score) ---\n        if trial is not None and epoch >= hpo_warmup_steps:\n            trial.report(val_f1, epoch)\n            if trial.should_prune():\n                logging.warning(f\"Optuna Trial {trial.number} pruned at epoch {epoch+1} (Val F1: {val_f1:.4f}).\")\n                del inputs, labels, outputs, loss\n                if torch.cuda.is_available(): torch.cuda.empty_cache()\n                raise optuna.TrialPruned() \n\n\n        # --- Early Stopping Check ---\n        if epochs_no_improve >= patience:\n            logging.info(f\"Early stopping triggered after {epoch+1} epochs (patience={patience}).\")\n            logging.info(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n            break \n\n\n    # --- End of Training Loop ---\n    total_duration = time.time() - total_start_time\n    logging.info(f\"--- Training loop finished for {model_name} in {timedelta(seconds=int(total_duration))} ---\")\n\n    if best_model_state is not None:\n        logging.info(f\"Loading best model weights from checkpoint (Val Loss: {best_val_loss:.4f}).\")\n        model.load_state_dict(best_model_state)\n    else:\n        logging.warning(\"No improvement in validation loss observed during training.\")\n        logging.warning(\"Model state from the last epoch will be used.\")\n        last_epoch_path = os.path.join(checkpoint_dir, f\"{model_name}_last_epoch.pth\")\n        torch.save(model.state_dict(), last_epoch_path)\n        logging.info(f\"Saved model state from last epoch to: {last_epoch_path}\")\n\n\n    # --- Plotting Training History ---\n    try:\n        sns.set_theme(style=\"darkgrid\") \n        num_epochs_trained = len(history['train_loss'])\n        if num_epochs_trained > 0:\n            epoch_range = range(1, num_epochs_trained + 1)\n\n            fig, axes = plt.subplots(1, 2, figsize=(16, 6)) \n\n            axes[0].plot(epoch_range, history['train_loss'], label='Train Loss', marker='o', linestyle='-', color='royalblue')\n            axes[0].plot(epoch_range, history['val_loss'], label='Val Loss', marker='x', linestyle='--', color='darkorange')\n            axes[0].set_title(f'{model_name} - Loss vs. Epochs')\n            axes[0].set_xlabel('Epoch')\n            axes[0].set_ylabel('Loss')\n            axes[0].legend()\n            axes[0].grid(True, linestyle=':')\n\n            axes[1].plot(epoch_range, history['train_acc'], label='Train Accuracy', marker='o', linestyle='-', color='royalblue')\n            axes[1].plot(epoch_range, history['val_acc'], label='Val Accuracy', marker='x', linestyle='--', color='darkorange')\n            axes[1].plot(epoch_range, history['val_f1'], label='Val F1 Score', marker='s', linestyle='-.', color='forestgreen')\n            axes[1].set_title(f'{model_name} - Accuracy & Val F1 vs. Epochs')\n            axes[1].set_xlabel('Epoch')\n            axes[1].set_ylabel('Metric Value')\n            axes[1].set_ylim(0.0, 1.05) \n            axes[1].legend()\n            axes[1].grid(True, linestyle=':')\n\n            fig.suptitle(f\"Training History: {model_name}\", fontsize=16)\n            plt.tight_layout(rect=[0, 0.03, 1, 0.95]) \n\n            plot_path = os.path.join(checkpoint_dir, f\"{model_name}_training_metrics.png\")\n            plt.savefig(plot_path, dpi=150)\n            plt.close(fig)\n            logging.info(f\"Training metrics plot saved to: {plot_path}\")\n        else:\n             logging.warning(\"No training history recorded (0 epochs trained). Skipping plot generation.\")\n\n    except Exception as e:\n        logging.error(f\"Failed to generate or save training plot for {model_name}: {e}\", exc_info=True)\n\n\n    return model, history","metadata":{"_uuid":"e342814b-e993-48fe-b46b-6a3baccb4b0c","_cell_guid":"8d7b62b0-8043-4913-b7a1-a0042c36f873","trusted":true,"id":"sNpOy-zmLNNb","executionInfo":{"status":"aborted","timestamp":1742977080464,"user_tz":-60,"elapsed":1,"user":{"displayName":"Khalil Krifi","userId":"13701701284808241864"}},"execution":{"iopub.status.busy":"2025-05-07T22:53:07.275621Z","iopub.execute_input":"2025-05-07T22:53:07.276038Z","iopub.status.idle":"2025-05-07T22:53:07.312664Z","shell.execute_reply.started":"2025-05-07T22:53:07.276001Z","shell.execute_reply":"2025-05-07T22:53:07.311260Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### 13. Optuna Objective Function","metadata":{"_uuid":"518604cb-21c5-452d-b53b-8904a269c5af","_cell_guid":"8b2f6b3d-fee3-49c5-aad6-de51e6a0d2ee","trusted":true,"collapsed":false,"id":"EDlPa0Wrgd2x","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell defines the `objective` function, which is the core component for hyperparameter optimization using the Optuna library. This function is called by Optuna for each trial.\nIts purpose and steps are:\n1.  **Receive Trial Object**: Takes an `optuna.Trial` object, `model_name`, `device`, and HPO-specific settings (`hpo_epochs`, `hpo_patience`, `hpo_warmup_steps`) as input.\n2.  **Suggest Hyperparameters**: Uses `trial.suggest_categorical()` and `trial.suggest_float()` to sample hyperparameters for the current trial. These include:\n    -   `dense_units`: Number of units in the classifier's dense layer.\n    -   `dropout`: Dropout rate in the classifier.\n    -   `learning_rate`: Learning rate for the optimizer.\n    -   `optimizer`: Type of optimizer (AdamW or Adam).\n    -   `batch_size`: Batch size for data loaders.\n    -   `unfreeze_layers`: Number of layers to unfreeze in the pre-trained base model (0 means fully frozen).\n3.  **Log Trial Parameters**: Prints the hyperparameters chosen for the current trial and the best F1 score found so far in the study.\n4.  **Setup for Training**:\n    -   Retrieves the appropriate `img_size` for the given `model_name`.\n    -   Calls `get_data_loaders` to create data loaders with the suggested `batch_size` and `img_size`.\n    -   Instantiates the `CustomModel` with the `model_name` and suggested `dense_units`, `dropout`, and `unfreeze_layers`.\n    -   Creates the optimizer (Adam or AdamW) with the suggested `learning_rate`, ensuring it targets all trainable parameters. If no trainable parameters are found (e.g., due to an issue with unfreezing logic or model setup), it prunes the trial.\n    -   Defines the loss function (`nn.BCEWithLogitsLoss`).\n5.  **Train Model**: Calls the `train_model` function (defined earlier) with the configured model, data loaders, optimizer, criterion, and HPO-specific settings (`hpo_epochs`, `hpo_patience`, `hpo_warmup_steps`). The `trial` object is passed to `train_model` to enable pruning.\n6.  **Evaluate Model**: After training (or if early stopping/pruning occurred), it calls `evaluate_model` to get performance metrics on the validation set using the best model state loaded by `train_model`.\n7.  **Log Trial Results**: Logs the duration of the trial and the validation metrics (Loss, Accuracy, F1, Precision, Recall).\n8.  **Return Metric**: Returns the validation F1 score (`val_f1`), which Optuna will attempt to maximize.\n9.  **Error Handling**:\n    -   Catches `optuna.TrialPruned` exceptions specifically to allow Optuna to handle pruning correctly.\n    -   Catches any other exceptions, logs the error, cleans up resources (deletes model, loaders, optimizer, criterion, and clears CUDA cache), and returns a poor value (0.0 for F1 score) to indicate trial failure to Optuna.","metadata":{}},{"cell_type":"code","source":"def objective(trial, model_name, device, hpo_epochs, hpo_patience, hpo_warmup_steps):\n    trial_start_time = time.time()\n    logging.info(f\"\\n--- Optuna Trial {trial.number} Start ({model_name}) ---\")\n\n    # --- Suggest Hyperparameters ---\n    dense_units = trial.suggest_categorical('dense_units', [128, 256, 512, 768])\n    dropout = trial.suggest_float('dropout', 0.1, 0.6, step=0.05) \n    learning_rate = trial.suggest_float('learning_rate', 5e-6, 1e-3, log=True) \n    optimizer_name = trial.suggest_categorical('optimizer', ['AdamW', 'Adam']) \n    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n    unfreeze_layers = trial.suggest_int('unfreeze_layers', 0, 3) \n\n    trial_params = trial.params\n    print(f\"  Trial {trial.number} Parameters:\")\n    for key, value in trial_params.items():\n         print(f\"    {key:<15}: {value}\")\n    try:\n        best_value_so_far = trial.study.best_value\n        print(f\"  Best Value ({trial.study.direction.name} F1) So Far: {best_value_so_far:.4f}\")\n    except ValueError: \n        print(f\"  Best Value ({trial.study.direction.name} F1) So Far: N/A (First trial)\")\n    img_size = model_img_sizes[model_name]\n    model, train_loader, val_loader, optimizer, criterion = None, None, None, None, None\n\n    try:\n        train_loader, val_loader, _ = get_data_loaders(model_name, batch_size, img_size, num_workers=2)\n\n        model = CustomModel(model_name=model_name,\n                           dense_units=dense_units,\n                           dropout=dropout,\n                           pretrained=True,\n                           unfreeze_layers=unfreeze_layers).to(device)\n\n        trainable_params = [p for p in model.parameters() if p.requires_grad]\n        if not trainable_params:\n            logging.error(f\"FATAL ERROR in Trial {trial.number}: No trainable parameters found for model {model_name} with unfreeze={unfreeze_layers}. Pruning.\")\n\n            raise optuna.TrialPruned(\"No trainable parameters found.\")\n\n\n        logging.info(f\"  Trial {trial.number}: Optimizing {len(trainable_params)} parameter groups ({sum(p.numel() for p in trainable_params):,} total parameters).\")\n\n        if optimizer_name == 'Adam':\n            optimizer = torch.optim.Adam(trainable_params, lr=learning_rate)\n        elif optimizer_name == 'AdamW':\n            optimizer = torch.optim.AdamW(trainable_params, lr=learning_rate)\n        else:\n            raise ValueError(f\"Unsupported optimizer suggested: {optimizer_name}\")\n\n        criterion = nn.BCEWithLogitsLoss()\n\n        model, history = train_model(model=model,\n                                      model_name=f\"{model_name}_trial_{trial.number}\", \n                                      train_loader=train_loader,\n                                      val_loader=val_loader,\n                                      criterion=criterion,\n                                      optimizer=optimizer,\n                                      epochs=hpo_epochs, \n                                      device=device,\n                                      trial=trial, \n                                      patience=hpo_patience,\n                                      hpo_warmup_steps=hpo_warmup_steps)\n\n        val_loss, val_accuracy, val_f1, val_precision, val_recall, _ = evaluate_model(model, val_loader, criterion, device)\n\n        trial_duration = time.time() - trial_start_time\n        logging.info(f\"--- Optuna Trial {trial.number} Finished [{timedelta(seconds=int(trial_duration))}] ---\")\n        logging.info(f\"  Validation Metrics: Loss={val_loss:.4f}, Acc={val_accuracy:.4f}, F1={val_f1:.4f}, Precision={val_precision:.4f}, Recall={val_recall:.4f}\")\n\n        return val_f1\n\n    except optuna.TrialPruned as e:\n        trial_duration = time.time() - trial_start_time\n        logging.info(f\"--- Optuna Trial {trial.number} Pruned [{timedelta(seconds=int(trial_duration))}] ---\")\n        del model, train_loader, val_loader, optimizer, criterion\n        if torch.cuda.is_available(): torch.cuda.empty_cache()\n        raise e \n\n    except Exception as e:\n        trial_duration = time.time() - trial_start_time\n        logging.error(f\"--- Optuna Trial {trial.number} Failed [{timedelta(seconds=int(trial_duration))}] ---\")\n        logging.error(f\"Error during Optuna trial {trial.number} for {model_name}: {e}\", exc_info=True)\n\n        if 'model' in locals() and model is not None: del model\n        if 'train_loader' in locals() and train_loader is not None: del train_loader\n        if 'val_loader' in locals() and val_loader is not None: del val_loader\n        if 'optimizer' in locals() and optimizer is not None: del optimizer\n        if 'criterion' in locals() and criterion is not None: del criterion\n        if torch.cuda.is_available(): torch.cuda.empty_cache()\n\n        return 0.0","metadata":{"_uuid":"9738e4d4-d1c3-43bd-80cf-ecd4d1c42845","_cell_guid":"fff87ca0-41c1-40ad-80e2-ff665ca60fde","trusted":true,"id":"EoBPubAYgeao","executionInfo":{"status":"aborted","timestamp":1742977080475,"user_tz":-60,"elapsed":1,"user":{"displayName":"Khalil Krifi","userId":"13701701284808241864"}},"execution":{"iopub.status.busy":"2025-05-07T22:53:07.313750Z","iopub.execute_input":"2025-05-07T22:53:07.314066Z","iopub.status.idle":"2025-05-07T22:53:07.339773Z","shell.execute_reply.started":"2025-05-07T22:53:07.314042Z","shell.execute_reply":"2025-05-07T22:53:07.338750Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### 14. Upload to Dataset","metadata":{"_uuid":"a528b129-6864-408d-ba25-303accb9c4c4","_cell_guid":"de18567e-c7cc-4c65-8883-f72b8728f0e3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell defines three helper functions related to saving and packaging results: `plot_training_history`, `plot_confusion_matrix`, and `upload_checkpoint_to_dataset`.\n\n1.  **`plot_training_history(history_dict, save_path, model_name)`**:\n    *   Takes a history dictionary (containing lists of 'train_loss', 'val_loss', 'train_acc', 'val_acc', 'val_f1', etc.), a save path, and the model name.\n    *   Generates and saves plots for:\n        *   Training vs. Validation Loss.\n        *   Training vs. Validation Accuracy.\n        *   Validation Metrics (F1, Precision, Recall) if available.\n    *   Plots are saved as PNG files in the `save_path` (typically the model's checkpoint directory).\n    *   Includes error handling for missing keys or other plotting issues.\n\n2.  **`plot_confusion_matrix(cm, save_path, model_name, class_names=None)`**:\n    *   Takes a confusion matrix (`cm`), a save path, model name, and optional class names.\n    *   Generates a heatmap visualization of the confusion matrix using `seaborn.heatmap`.\n    *   Saves the plot as a PNG file in the `save_path`.\n\n3.  **`upload_checkpoint_to_dataset(model_name)`**:\n    *   This is the main function for packaging and making results accessible.\n    *   **Load Metrics and Generate Plots**:\n        *   Constructs the path to the model's checkpoint directory and the results JSON file (e.g., `_test_results.json`).\n        *   If the metrics file exists, it loads the JSON data.\n        *   Extracts `training_history` and `confusion_matrix` from the loaded metrics.\n        *   Calls `plot_training_history` and `plot_confusion_matrix` to generate plots and save them directly into the model's checkpoint directory.\n    *   **File Collection**: Lists all relevant files in the checkpoint directory (`.pth`, `.pt`, `.onnx` for models; `.json` for metrics; `.png` for plots).\n    *   **Environment-Specific Handling**:\n        *   **Kaggle Environment**:\n            *   Creates a zip file (e.g., `{model_name}_checkpoints_and_plots.zip`) in `/kaggle/working/` containing all collected files from the checkpoint directory.\n            *   Verifies the zip file creation and logs its size and contents.\n            *   Prints instructions for downloading the zip file via the Kaggle UI ('Data' tab -> 'Output').\n            *   Attempts to provide a direct download link using `IPython.display.FileLink` as a secondary method.\n        *   **Colab Environment**:\n            *   Copies all collected files from the local checkpoint directory to a corresponding directory in Google Drive (`/content/drive/MyDrive/checkpoints/SignatureVerification/{model_name}`).\n            *   Logs the number of files copied and the destination path.\n        *   **Local Environment (Unknown)**:\n            *   Copies all collected files to a local directory (e.g., `~/checkpoints/SignatureVerification/{model_name}`).\n            *   Logs the number of files copied and the destination path.\n    *   Includes error handling for various stages like JSON decoding, file operations, and zip creation.","metadata":{}},{"cell_type":"code","source":"def plot_training_history(history_dict, save_path, model_name):\n    if not history_dict:\n        logger.warning(f\"No training history data provided for {model_name}. Skipping history plots.\")\n        return\n\n    try:\n        epochs = range(1, len(history_dict['train_loss']) + 1)\n\n        # --- Loss Plot ---\n        plt.figure(figsize=(10, 5))\n        plt.plot(epochs, history_dict['train_loss'], 'bo-', label='Training Loss')\n        plt.plot(epochs, history_dict['val_loss'], 'ro-', label='Validation Loss')\n        plt.title(f'{model_name} - Training & Validation Loss')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.grid(True)\n        plt.tight_layout()\n        plt.savefig(os.path.join(save_path, f'{model_name}_loss_plot.png'))\n        plt.close() \n        logger.info(f\"Saved loss plot for {model_name}\")\n\n        # --- Accuracy Plot ---\n        plt.figure(figsize=(10, 5))\n        plt.plot(epochs, history_dict['train_acc'], 'bo-', label='Training Accuracy')\n        plt.plot(epochs, history_dict['val_acc'], 'ro-', label='Validation Accuracy')\n        plt.title(f'{model_name} - Training & Validation Accuracy')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.legend()\n        plt.grid(True)\n        plt.tight_layout()\n        plt.savefig(os.path.join(save_path, f'{model_name}_accuracy_plot.png'))\n        plt.close()\n        logger.info(f\"Saved accuracy plot for {model_name}\")\n\n        # --- Validation Metrics Plot (F1, Precision, Recall) ---\n        if all(k in history_dict for k in ['val_f1', 'val_precision', 'val_recall']):\n             plt.figure(figsize=(10, 5))\n             plt.plot(epochs, history_dict['val_f1'], 'go-', label='Validation F1-Score')\n             plt.plot(epochs, history_dict['val_precision'], 'yo-', label='Validation Precision')\n             plt.plot(epochs, history_dict['val_recall'], 'mo-', label='Validation Recall')\n             plt.title(f'{model_name} - Validation Metrics')\n             plt.xlabel('Epochs')\n             plt.ylabel('Score')\n             plt.legend()\n             plt.grid(True)\n             plt.tight_layout()\n             plt.savefig(os.path.join(save_path, f'{model_name}_val_metrics_plot.png'))\n             plt.close()\n             logger.info(f\"Saved validation metrics plot for {model_name}\")\n        else:\n             logger.warning(f\"Missing some validation metrics (F1, Precision, Recall) for {model_name}. Skipping val metrics plot.\")\n\n    except KeyError as e:\n        logger.error(f\"Missing key in history_dict for {model_name}: {e}. Cannot generate plots.\")\n    except Exception as e:\n        logger.error(f\"Error plotting training history for {model_name}: {e}\", exc_info=True)\n\n\ndef plot_confusion_matrix(cm, save_path, model_name, class_names=None):\n    if not cm:\n        logger.warning(f\"No confusion matrix data provided for {model_name}. Skipping CM plot.\")\n        return\n\n    try:\n        cm_array = np.array(cm)\n        if not class_names:\n             if cm_array.shape == (2,2):\n                  class_names = ['Class 0', 'Class 1']\n             else: \n                  class_names = [f'Class {i}' for i in range(cm_array.shape[0])]\n\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm_array, annot=True, fmt='d', cmap='Blues',\n                    xticklabels=class_names, yticklabels=class_names, cbar=False)\n        plt.title(f'{model_name} - Confusion Matrix')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.tight_layout()\n        plt.savefig(os.path.join(save_path, f'{model_name}_confusion_matrix.png'))\n        plt.close()\n        logger.info(f\"Saved confusion matrix plot for {model_name}\")\n\n    except Exception as e:\n        logger.error(f\"Error plotting confusion matrix for {model_name}: {e}\", exc_info=True)\n\n\n\ndef upload_checkpoint_to_dataset(model_name):\n    logger.info(f\"Starting upload/packaging process for model: {model_name}\")\n    checkpoint_dir = os.path.join(CHECKPOINT_BASE_DIR, model_name)\n    if not os.path.exists(checkpoint_dir):\n        logger.warning(f\"Checkpoint directory {checkpoint_dir} does not exist.\")\n        return\n\n    # --- Load Metrics and Generate Plots ---\n    metrics_file_path = results_path \n    metrics_data = None\n    if os.path.exists(metrics_file_path):\n        try:\n            with open(metrics_file_path, 'r') as f:\n                metrics_data = json.load(f)\n            logger.info(f\"Loaded metrics data from {metrics_file_path}\")\n\n            history = metrics_data.get('training_history')\n            cm = metrics_data.get('confusion_matrix')\n            plot_training_history(history, checkpoint_dir, model_name)\n            plot_confusion_matrix(cm, checkpoint_dir, model_name) \n\n        except json.JSONDecodeError:\n            logger.error(f\"Error decoding JSON from {metrics_file_path}. Cannot generate plots.\")\n        except Exception as e:\n            logger.error(f\"Error processing metrics or plotting for {model_name}: {e}\", exc_info=True)\n    else:\n        logger.warning(f\"Metrics file {metrics_file_path} not found. Skipping plot generation.\")\n    # --- End Plot Generation ---\n\n\n    try:\n        checkpoint_files = [\n            f for f in os.listdir(checkpoint_dir)\n            if f.endswith(('.pth', '.pt', '.onnx', '.json', '.png'))\n        ]\n        if not checkpoint_files:\n            logger.warning(f\"No checkpoint-related files (.pth, .pt, .onnx, .json, .png) found in {checkpoint_dir}\")\n        files_to_zip = []\n        for file in checkpoint_files:\n             src = os.path.join(checkpoint_dir, file)\n             if os.path.isfile(src):\n                 files_to_zip.append(file)\n             else:\n                 logger.warning(f\"File '{file}' listed but not found at '{src}'. Skipping.\")\n\n        if not files_to_zip:\n             logger.error(f\"No valid files found to zip in {checkpoint_dir}\")\n             return\n\n        # --- Environment Specific Handling (Kaggle, Colab, Local) ---\n        if ENVIRONMENT == 'kaggle':\n            zip_filename = f'{model_name}_checkpoints_and_plots.zip' \n            zip_path = os.path.join('/kaggle/working', zip_filename)\n            logger.info(f\"Creating zip file at: {zip_path}\")\n\n            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n                for file in files_to_zip:\n                    src = os.path.join(checkpoint_dir, file)\n                    zipf.write(src, arcname=file) \n            logger.info(f\"Zip file creation process finished for {zip_path}\")\n\n            time.sleep(1)\n            if os.path.exists(zip_path):\n                file_size = os.path.getsize(zip_path)\n                logger.info(f\"Verified zip file exists: {zip_path} (Size: {file_size} bytes)\")\n                print(f\"\\nSUCCESS: Zip file created at: {zip_path} (Size: {file_size} bytes)\")\n                print(f\"   Contains: {files_to_zip}\") \n\n                # --- Primary Download Method: Kaggle UI ---\n                print(\"\\n--- DOWNLOAD INSTRUCTIONS (Primary Method) ---\")\n                print(f\"1. Go to the 'Data' tab -> 'Output' section in the right-hand panel.\")\n                print(f\"2. Find the file named '{zip_filename}'.\")\n                print(f\"3. Click the download icon.\")\n                print(f\"   (Note: May take a minute to appear after cell finishes.)\")\n\n                # --- Secondary Method: FileLink (Attempt) ---\n                print(\"\\n--- Direct Link (Secondary Method - May Not Work) ---\")\n                try:\n                    display(FileLink(zip_path))\n                    print(f\"-> If a link for '{zip_filename}' appeared above, try clicking it.\")\n                    print(f\"-> If it fails, use the Primary Method (Kaggle UI Output tab).\")\n                except Exception as e:\n                    logger.error(f\"Error generating FileLink: {e}\", exc_info=True)\n                    print(f\"   - Could not generate direct link. Use the Primary Method.\")\n\n            else:\n                logger.error(f\"CRITICAL ERROR: Zip file NOT found at '{zip_path}' after creation attempt!\")\n\n        elif ENVIRONMENT == 'colab':\n            drive_dir = f\"/content/drive/MyDrive/checkpoints/SignatureVerification/{model_name}\"\n            os.makedirs(drive_dir, exist_ok=True)\n            copied_files_count = 0\n            logger.info(f\"Copying {len(files_to_zip)} files to Google Drive...\")\n            for file in files_to_zip:\n                src = os.path.join(checkpoint_dir, file)\n                dst = os.path.join(drive_dir, file)\n                try:\n                    shutil.copy(src, dst)\n                    copied_files_count += 1\n                except Exception as e:\n                     logger.error(f\"Failed to copy {src} to {dst}: {e}\", exc_info=True)\n            logger.info(f\"{copied_files_count} file(s) copied to Google Drive: {drive_dir}\")\n            print(f\"Checkpoint files & plots saved to Google Drive at: {drive_dir}\")\n\n\n        else: \n            local_dir = os.path.expanduser(f\"~/checkpoints/SignatureVerification/{model_name}\")\n            os.makedirs(local_dir, exist_ok=True)\n            copied_files_count = 0\n            logger.info(f\"Copying {len(files_to_zip)} files to local storage...\")\n            for file in files_to_zip:\n                src = os.path.join(checkpoint_dir, file)\n                dst = os.path.join(local_dir, file)\n                try:\n                    shutil.copy(src, dst)\n                    copied_files_count += 1\n                except Exception as e:\n                     logger.error(f\"Failed to copy {src} to {dst}: {e}\", exc_info=True)\n            logger.info(f\"{copied_files_count} file(s) copied locally to: {local_dir}\")\n            print(f\"Checkpoints & plots saved locally to: {local_dir}\")\n\n\n    except Exception as e:\n        logger.error(f\"Failed to process checkpoints/plots for {model_name}: {str(e)}\", exc_info=True)\n        print(f\"\\nERROR: An unexpected error occurred during checkpoint/plot processing: {e}\")\n\n","metadata":{"_uuid":"f291e2a6-d338-4576-a0c6-95aef3cb3eeb","_cell_guid":"925f3844-00ee-42bc-a558-faaa0d7a30f7","trusted":true,"id":"BYQcpPWUIskM","executionInfo":{"status":"aborted","timestamp":1742977080567,"user_tz":-60,"elapsed":1,"user":{"displayName":"Khalil Krifi","userId":"13701701284808241864"}},"execution":{"iopub.status.busy":"2025-05-07T22:53:07.340907Z","iopub.execute_input":"2025-05-07T22:53:07.341317Z","iopub.status.idle":"2025-05-07T22:53:07.368861Z","shell.execute_reply.started":"2025-05-07T22:53:07.341278Z","shell.execute_reply":"2025-05-07T22:53:07.367958Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# *  Run the Models and generate checkpoints :","metadata":{"_uuid":"667956e6-74bb-49bd-a4c1-24658da72d8e","_cell_guid":"d0723845-981a-4698-95c0-6d42cf5c65ad","trusted":true,"collapsed":false,"id":"vhxVaWePg2Vy","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell is the main execution block of the notebook. It orchestrates the entire process of hyperparameter optimization (HPO), final model training, evaluation, and results storage for each model specified in `models_to_try`.\n\n**Overall Workflow**:\n1.  **Initialization**: Records the overall start time and initializes a `results` dictionary to store outcomes for each model.\n2.  **Configuration**: Sets parameters for:\n    -   **Optuna HPO**: `n_trials_optuna` (number of trials per model), `hpo_timeout_seconds` (timeout for HPO study), `hpo_epochs` (max epochs per HPO trial), `hpo_patience` (early stopping patience within HPO), `hpo_warmup_steps` (epochs before Optuna pruning starts).\n    -   **Final Training**: `final_train_epochs` (max epochs for the final run), `final_train_patience` (early stopping patience for the final run).\n    -   **Loss Function**: Defines `criterion` as `nn.BCEWithLogitsLoss()` for binary classification.\n3.  **Model Loop**: Iterates through each `model_name` in the `models_to_try` list.\n    -   **Setup**: Records model start time, logs the current model being processed, and creates a specific checkpoint directory for it.\n    -   **Hyperparameter Tuning (Optuna)**:\n        -   Logs the start of HPO and its settings.\n        -   Creates an Optuna `study` (or loads an existing one from the `OPTUNA_DB_PATH`) with a `MedianPruner`. The study aims to `maximize` the validation F1 score.\n        -   Calls `study.optimize()`, passing the `objective` function (defined in Cell 13) along with the model name, device, and HPO settings.\n        -   Handles potential errors during study creation or optimization.\n        -   After HPO, retrieves the `best_trial`, `best_params`, and `best_value` (best validation F1). Logs these best HPO results.\n        -   Handles cases where no successful trials or no best trial is found.\n    -   **Final Training with Best Parameters**:\n        -   Logs the start of final training and its settings.\n        -   Extracts the best hyperparameters (batch size, learning rate, dense units, dropout, optimizer name, unfreeze layers) from `best_params`.\n        -   Gets data loaders using the best batch size.\n        -   Builds the `CustomModel` using the best hyperparameters (including `unfreeze_layers`).\n        -   Creates the final optimizer based on the best optimizer name and learning rate, ensuring it targets all trainable parameters of the final model.\n        -   Calls `train_model` to train this final model (without Optuna trial/pruning).\n    -   **Final Evaluation on Test Set**:\n        -   Evaluates the trained final model on the `test_loader` using `evaluate_model`.\n        -   Stores all results (HPO params, HPO F1 score, final training epochs, test metrics like loss, accuracy, F1, precision, recall, confusion matrix, and the full training history) in the `results` dictionary for the current model.\n        -   Logs the test set metrics.\n        -   Saves these detailed results to a JSON file (e.g., `{model_name}_test_results.json`) in the model's checkpoint directory.\n    -   **Resource Cleanup**: Deletes model objects, data loaders, optimizers, and clears CUDA cache to free up memory before processing the next model.\n    -   **Error Handling**: Includes a broad `try-except` block for each model to catch any unexpected errors, log them, store an error status in `results`, and clean up resources.\n    -   Logs the total processing time for the current model.","metadata":{}},{"cell_type":"code","source":"overall_start_time = time.time()\nresults = {} \n\n# --- Configuration ---\nn_trials_optuna = 30  \nhpo_timeout_seconds = 36000 \nhpo_epochs = 8       \nhpo_patience = 3     \nhpo_warmup_steps = 2 \n\nfinal_train_epochs = 25 \nfinal_train_patience = 5  \n\ncriterion = nn.BCEWithLogitsLoss()\n\n# --- Model Loop ---\nfor model_name in models_to_try:\n    model_start_time = time.time()\n    logging.info(f\"\\n{'='*70}\\nProcessing Model: {model_name}\\n{'='*70}\")\n\n    model_checkpoint_dir = os.path.join(CHECKPOINT_BASE_DIR, model_name)\n    os.makedirs(model_checkpoint_dir, exist_ok=True) \n\n    try:\n        # ===================================\n        # 1. Hyperparameter Tuning (Optuna)\n        # ===================================\n        hpo_start_time = time.time()\n        logging.info(f\"--- Starting Hyperparameter Tuning for {model_name} ---\")\n        logging.info(f\"  Optuna Settings: Max Trials={n_trials_optuna}, Timeout={timedelta(seconds=hpo_timeout_seconds)}, \"\n                     f\"Epochs/Trial={hpo_epochs}, Patience/Trial={hpo_patience}, Pruning Warmup={hpo_warmup_steps}\")\n\n        study_name = f\"sig_verify_{model_name}_hpo_v3\" \n        study = None\n        best_params = None\n        best_value = None\n\n        try:\n            pruner = optuna.pruners.MedianPruner(n_warmup_steps=hpo_warmup_steps, n_min_trials=5)\n\n            study = optuna.create_study(\n                direction='maximize',\n                study_name=study_name,\n                storage=OPTUNA_DB_PATH,\n                pruner=pruner,\n                load_if_exists=True\n            )\n\n            n_existing_trials = len([t for t in study.trials if t.state != optuna.trial.TrialState.WAITING])\n            logging.info(f\"Optuna study '{study_name}': Loaded with {n_existing_trials} existing trials.\")\n\n            trials_to_run = n_trials_optuna - n_existing_trials\n            if trials_to_run <= 0:\n                logging.info(f\"Study already has {n_existing_trials} >= {n_trials_optuna} trials. Using existing best params.\")\n            else:\n                logging.info(f\"Running {trials_to_run} new Optuna trials...\")\n                study.optimize(lambda trial: objective(trial, model_name, device,\n                                                       hpo_epochs, hpo_patience, hpo_warmup_steps),\n                               n_trials=trials_to_run,\n                               timeout=hpo_timeout_seconds,\n                               gc_after_trial=True)\n\n        except Exception as hpo_e:\n             logging.error(f\"Error during Optuna study creation or optimization for {model_name}: {hpo_e}\", exc_info=True)\n             results[model_name] = {'error': f'Optuna HPO failed: {hpo_e}'}\n             del study\n             if torch.cuda.is_available(): torch.cuda.empty_cache()\n             continue\n\n        hpo_duration = time.time() - hpo_start_time\n        logging.info(f\"--- Optuna Tuning Complete for {model_name} in {timedelta(seconds=int(hpo_duration))} ---\")\n        if not study or not hasattr(study, 'best_trial') or study.best_trial is None:\n            completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n            if not completed_trials:\n                 logging.error(f\"Optuna study for {model_name} completed but had NO successful trials. Cannot proceed.\")\n                 results[model_name] = {'error': 'Optuna study finished with no successful trials.'}\n            else:\n                 logging.warning(f\"Optuna study for {model_name} finished, but 'study.best_trial' is None. Trying to find best completed trial.\")\n                 best_completed_trial = max(completed_trials, key=lambda t: t.value if t.value is not None else -float('inf'))\n                 if best_completed_trial.value is not None:\n                      best_trial = best_completed_trial\n                      logging.info(f\"Manually selected best completed trial #{best_trial.number} with value {best_trial.value:.4f}\")\n                 else:\n                      logging.error(\"Could not find a best trial among completed trials. Cannot proceed.\")\n                      results[model_name] = {'error': 'Optuna study failed to determine a best trial.'}\n                      del study\n                      if torch.cuda.is_available(): torch.cuda.empty_cache()\n                      continue\n\n            del study\n            if torch.cuda.is_available(): torch.cuda.empty_cache()\n            continue \n\n        best_trial = study.best_trial\n        best_params = best_trial.params\n        best_value = best_trial.value\n\n        logging.info(f\"  Best Trial Found: #{best_trial.number} (Value: {best_value:.4f})\")\n        logging.info(f\"  Best Hyperparameters:\")\n        for key, value in best_params.items():\n            logging.info(f\"    {key:<15}: {value}\")\n\n        # ========================================\n        # 2. Final Training with Best Parameters\n        # ========================================\n        final_training_start_time = time.time()\n        logging.info(f\"\\n--- Starting Final Training: {model_name} with Best Params ---\")\n        logging.info(f\"  Settings: Max Epochs={final_train_epochs}, Patience={final_train_patience}\")\n\n        img_size = model_img_sizes[model_name]\n        final_batch_size = best_params['batch_size']\n        final_lr = best_params['learning_rate']\n        final_dense_units = best_params['dense_units']\n        final_dropout = best_params['dropout']\n        final_optimizer_name = best_params['optimizer']\n        final_unfreeze_layers = best_params['unfreeze_layers']\n\n        train_loader, val_loader, test_loader = get_data_loaders(model_name, final_batch_size, img_size, num_workers=2) \n\n        final_model = CustomModel(model_name=model_name,\n                                 dense_units=final_dense_units,\n                                 dropout=final_dropout,\n                                 pretrained=True,\n                                 unfreeze_layers=final_unfreeze_layers).to(device)\n\n        trainable_params_final = [p for p in final_model.parameters() if p.requires_grad]\n        if not trainable_params_final:\n            logging.error(f\"FATAL: No trainable parameters found for the FINAL model {model_name}! Skipping training.\")\n            results[model_name] = {'error': 'Final model build resulted in no trainable parameters.'}\n            del final_model, train_loader, val_loader, test_loader, study\n            if torch.cuda.is_available(): torch.cuda.empty_cache()\n            continue\n\n        logging.info(f\"Final model: Optimizing {len(trainable_params_final)} parameter groups ({sum(p.numel() for p in trainable_params_final):,} total parameters).\")\n\n        if final_optimizer_name == 'Adam':\n            final_optimizer = torch.optim.Adam(trainable_params_final, lr=final_lr)\n        elif final_optimizer_name == 'AdamW':\n            final_optimizer = torch.optim.AdamW(trainable_params_final, lr=final_lr) \n        else:\n            logging.warning(f\"Unsupported optimizer '{final_optimizer_name}' found in best params. Defaulting to AdamW.\")\n            final_optimizer = torch.optim.AdamW(trainable_params_final, lr=final_lr)\n        final_model, history = train_model(model=final_model,\n                                           model_name=model_name,\n                                           train_loader=train_loader,\n                                           val_loader=val_loader,\n                                           criterion=criterion,\n                                           optimizer=final_optimizer,\n                                           epochs=final_train_epochs,\n                                           device=device,\n                                           patience=final_train_patience,\n                                           trial=None,\n                                           hpo_warmup_steps=0) \n\n        final_training_duration = time.time() - final_training_start_time\n        logging.info(f\"--- Final Training Complete for {model_name} in {timedelta(seconds=int(final_training_duration))} ---\")\n\n        # ===================================\n        # 3. Final Evaluation on Test Set\n        # ===================================\n        eval_start_time = time.time()\n        logging.info(f\"\\n--- Evaluating Final Model ({model_name}) on Test Set ---\")\n        test_loss, test_accuracy, test_f1, test_precision, test_recall, conf_matrix = evaluate_model(\n            final_model, test_loader, criterion, device\n        )\n        eval_duration = time.time() - eval_start_time\n        logging.info(f\"--- Test Set Evaluation Complete in {eval_duration:.2f} seconds ---\")\n\n\n        results[model_name] = {\n            'status': 'Success',\n            'best_params_from_hpo': best_params,\n            'best_val_f1_hpo': best_value,\n            'final_train_epochs_run': len(history.get('train_loss', [])),\n            'test_loss': test_loss,\n            'test_accuracy': test_accuracy,\n            'test_f1': test_f1,\n            'test_precision': test_precision,\n            'test_recall': test_recall,\n            'confusion_matrix': conf_matrix.tolist(), \n            'training_history': history \n        }\n\n        logging.info(f\"--- Test Set Metrics for Final Trained {model_name} ---\")\n        logging.info(f\"  Accuracy:  {test_accuracy:.4f}\")\n        logging.info(f\"  F1 Score:  {test_f1:.4f}\")\n        logging.info(f\"  Precision: {test_precision:.4f}\")\n        logging.info(f\"  Recall:    {test_recall:.4f}\")\n        logging.info(f\"  Loss:      {test_loss:.4f}\")\n\n        results_path = os.path.join(model_checkpoint_dir, f\"{model_name}_test_results.json\")\n        try:\n             serializable_results = copy.deepcopy(results[model_name])\n             if 'confusion_matrix' in serializable_results and isinstance(serializable_results['confusion_matrix'], np.ndarray):\n                  serializable_results['confusion_matrix'] = serializable_results['confusion_matrix'].tolist()\n             serializable_results['model_name'] = model_name\n\n             with open(results_path, 'w') as f:\n                  json.dump(serializable_results, f, indent=4)\n             logging.info(f\"Test results saved to: {results_path}\")\n        except Exception as json_e:\n             logging.error(f\"Error saving test results to JSON {results_path}: {json_e}\", exc_info=True)\n\n        # ===================================\n        # 5. Clean up Resources\n        # ===================================\n        logging.info(f\"Cleaning up resources for {model_name}...\")\n        del final_model, train_loader, val_loader, test_loader, final_optimizer, history, study, trainable_params_final\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        logging.info(f\"Resources cleaned up for {model_name}.\")\n\n\n    except Exception as model_e:\n        logging.error(f\"!!!!!!!! UNEXPECTED ERROR processing model {model_name}: {model_e} !!!!!!!!\", exc_info=True)\n        results[model_name] = {'status': 'Failed', 'error': str(model_e)}\n\n        if 'final_model' in locals(): del final_model\n        if 'train_loader' in locals(): del train_loader\n        if 'val_loader' in locals(): del val_loader\n        if 'test_loader' in locals(): del test_loader\n        if 'final_optimizer' in locals(): del final_optimizer\n        if 'study' in locals(): del study\n        if 'history' in locals(): del history\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        logging.info(f\"Resources cleaned up for {model_name} after error.\")\n\n\n    model_duration = time.time() - model_start_time\n    logging.info(f\"--- Completed ALL processing for {model_name} in {timedelta(seconds=int(model_duration))} ---\")","metadata":{"_uuid":"0d12eaae-3d3d-4ed6-94d5-bda12265bdd6","_cell_guid":"ce6096a3-53f9-49c6-88d7-d06ff8ac1dc2","trusted":true,"id":"eCmGLRh-g6Yc","executionInfo":{"status":"aborted","timestamp":1742977080485,"user_tz":-60,"elapsed":1,"user":{"displayName":"Khalil Krifi","userId":"13701701284808241864"}},"execution":{"iopub.status.busy":"2025-05-07T22:53:07.369876Z","iopub.execute_input":"2025-05-07T22:53:07.370122Z"}},"outputs":[{"name":"stdout","text":"05/07 22:53:07 - \n======================================================================\nProcessing Model: ViT_Base\n======================================================================\n05/07 22:53:07 - --- Starting Hyperparameter Tuning for ViT_Base ---\n05/07 22:53:07 -   Optuna Settings: Max Trials=30, Timeout=10:00:00, Epochs/Trial=8, Patience/Trial=3, Pruning Warmup=2\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-05-07 22:53:09,587] A new study created in RDB with name: sig_verify_ViT_Base_hpo_v3\n","output_type":"stream"},{"name":"stdout","text":"05/07 22:53:09 - Optuna study 'sig_verify_ViT_Base_hpo_v3': Loaded with 0 existing trials.\n05/07 22:53:09 - Running 30 new Optuna trials...\n05/07 22:53:09 - \n--- Optuna Trial 0 Start (ViT_Base) ---\n  Trial 0 Parameters:\n    dense_units    : 512\n    dropout        : 0.1\n    learning_rate  : 4.040740331614598e-05\n    optimizer      : Adam\n    batch_size     : 64\n    unfreeze_layers: 1\n  Best Value (MAXIMIZE F1) So Far: N/A (First trial)\n05/07 22:53:09 - Creating DataLoaders for ViT_Base (Img Size: (224, 224), Batch Size: 64, Num Workers: 2)...\n05/07 22:53:09 - Dataset sizes: Train=4318, Validation=925, Test=928\n05/07 22:53:09 -   Train class distribution: Real=2231, Forged=2087\n05/07 22:53:09 - DataLoaders created successfully for ViT_Base.\n05/07 22:53:09 - Initializing CustomModel: Base='ViT_Base', DenseUnits=512, Dropout=0.10, Unfreeze=1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31454a08024648b8827621682d659cfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7062e65ef3549c58418b380c1546243"}},"metadata":{}},{"name":"stdout","text":"05/07 22:53:12 - Loaded 'google/vit-base-patch16-224' from HuggingFace Transformers. Reported features: 768\n05/07 22:53:13 - Verified that model 'ViT_Base' outputs 768 features as expected.\n05/07 22:53:13 - Initially froze 85,798,656 parameters in the base model.\n05/07 22:53:13 - Unfroze ViT encoder layer 11\n05/07 22:53:13 - Base model state after unfreeze: 7,087,872 trainable params, 78,710,784 frozen params.\n05/07 22:53:13 - Classifier head created with 395,265 trainable parameters.\n05/07 22:53:13 -   Trial 0: Optimizing 22 parameter groups (7,483,137 total parameters).\n05/07 22:53:13 - Best model checkpoint (based on val loss) will be saved to: /kaggle/working/checkpoints/SignatureVerification/ViT_Base_trial_0/ViT_Base_trial_0_best_val_loss.pth\n05/07 22:53:13 - --- Starting Training: ViT_Base_trial_0 for 8 epochs (Patience: 3) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/8 Train:   0%|          | 0/68 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/15 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"05/07 22:56:20 - Epoch 1/8 | Train Loss: 0.6611 | Train Acc: 0.6070 | Val Loss: 0.6076 | Val Acc: 0.6649 | Val F1: 0.6586 | Time: 0:03:07\n05/07 22:56:21 -   -> Val loss improved to 0.6076. Saved best model checkpoint.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/8 Train:   0%|          | 0/68 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/15 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"05/07 22:59:39 - Epoch 2/8 | Train Loss: 0.5823 | Train Acc: 0.6964 | Val Loss: 0.5685 | Val Acc: 0.7049 | Val F1: 0.6844 | Time: 0:03:18\n05/07 22:59:40 -   -> Val loss improved to 0.5685. Saved best model checkpoint.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/8 Train:   0%|          | 0/68 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/15 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"05/07 23:03:01 - Epoch 3/8 | Train Loss: 0.5462 | Train Acc: 0.7228 | Val Loss: 0.5369 | Val Acc: 0.7373 | Val F1: 0.7315 | Time: 0:03:20\n05/07 23:03:02 -   -> Val loss improved to 0.5369. Saved best model checkpoint.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/8 Train:   0%|          | 0/68 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3ee0ad95a2f4c58b45e6c2be2923521"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# Print final results :","metadata":{"_uuid":"0fa01870-4640-4ab6-a23b-b6c58d6cff91","_cell_guid":"6301f2cf-53dd-494d-a54d-9c1254e8b9e0","trusted":true,"collapsed":false,"id":"V7amgXwht_-y","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This cell is responsible for summarizing and displaying the results obtained from the main experimentation loop (Cell 20).\nIt performs the following actions:\n1.  **Log Total Execution Time**: Calculates and logs the total time taken for the entire script to run.\n2.  **Iterate Through Results**:\n    -   Sorts the model names alphabetically from the `results` dictionary for consistent output.\n    -   For each `model_name`:\n        -   Prints a header for the model.\n        -   **Status Check**: Checks if an error occurred during the processing of this model. If so, prints \"FAILED\" and the error message.\n        -   **Success Case**: If processing was successful:\n            -   Prints \"Success\".\n            -   Displays the best validation F1 score achieved during Hyperparameter Optimization (HPO) and the corresponding best hyperparameters.\n            -   Prints the key test set metrics (Accuracy, F1 Score, Precision, Recall, Loss) achieved by the final trained model.\n            -   **Plot Confusion Matrix**: If a confusion matrix is available in the results, it's plotted using `seaborn.heatmap` (with 'Forged' and 'Real' labels) and displayed.\n            -   **Plot Training History**: If training history (loss and accuracy curves) is available, it's plotted using `matplotlib`. Two subplots are created: one for loss (training vs. validation) and one for accuracy (training vs. validation) over epochs. These plots are then displayed.\n3.  **Final Log Message**: Logs \"--- Script Finished ---\".\n\nThis cell provides a comprehensive overview of each model's performance, including both HPO insights and final test evaluation, along with visualizations to aid in analysis.","metadata":{}},{"cell_type":"code","source":"overall_duration = time.time() - overall_start_time\nlogging.info(f\"\\n{'='*60}\\nTotal Script Execution Time: {timedelta(seconds=int(overall_duration))}\\n{'='*60}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"      Summary of Test Results (from Initial Training Run)\")\nprint(\"=\"*60)\nsuccessful_models_initial = 0\nsorted_model_names = sorted(results.keys())\n\nfor model_name in sorted_model_names:\n    metrics = results[model_name]\n    print(f\"\\n--- Results for: {model_name} (Initial Run) ---\")\n    if 'error' in metrics:\n        print(f\"  Status: FAILED\")\n        print(f\"  Error: {metrics['error']}\")\n        continue\n\n    successful_models_initial += 1\n    print(f\"  Status: Success\")\n    print(f\"  Best Validation F1 (HPO): {metrics.get('best_val_f1_hpo', 'N/A'):.4f}\")\n    print(f\"  Best Hyperparameters (from HPO): {metrics.get('best_params_from_hpo', 'N/A')}\")\n    print(\"-\" * 30)\n    print(f\"  Test Accuracy:       | {metrics.get('test_accuracy', -1):.4f} |\")\n    print(f\"  Test F1 Score:       | {metrics.get('test_f1', -1):.4f} |\")\n    print(f\"  Test Precision:      | {metrics.get('test_precision', -1):.4f} |\")\n    print(f\"  Test Recall:         | {metrics.get('test_recall', -1):.4f} |\")\n    print(f\"  Test Loss:           | {metrics.get('test_loss', -1):.4f} |\")\n    print(\"-\" * 30)\n\n    # --- Plot Confusion Matrix (from initial run's test eval) ---\n    if 'confusion_matrix' in metrics:\n        conf_matrix = np.array(metrics['confusion_matrix'])\n        plt.figure(figsize=(6, 5))\n        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens', \n                    xticklabels=['Forged', 'Real'], yticklabels=['Forged', 'Real'], cbar=False)\n        plt.title(f'Confusion Matrix (Initial Test Eval) - {model_name}')\n        plt.xlabel('Predicted Label')\n        plt.ylabel('True Label')\n        plt.tight_layout()\n        plt.show()\n\n\n    # --- Plot Training History (Loss and Accuracy Curves) ---\n    if 'training_history' in metrics:\n        history = metrics['training_history']\n        epochs_ran = len(history.get('train_loss', []))\n        if epochs_ran > 0:\n            epoch_list = range(1, epochs_ran + 1)\n            plt.figure(figsize=(12, 5))\n\n            plt.subplot(1, 2, 1)\n            plt.plot(epoch_list, history['train_loss'], 'bo-', label='Training Loss')\n            plt.plot(epoch_list, history['val_loss'], 'ro-', label='Validation Loss')\n            plt.title(f'Loss vs. Epochs - {model_name}')\n            plt.xlabel('Epochs')\n            plt.ylabel('Loss')\n            plt.legend()\n            plt.grid(True)\n\n\n            plt.subplot(1, 2, 2)\n            plt.plot(epoch_list, history['train_acc'], 'bo-', label='Training Accuracy')\n            plt.plot(epoch_list, history['val_acc'], 'ro-', label='Validation Accuracy')\n            plt.title(f'Accuracy vs. Epochs - {model_name}')\n            plt.xlabel('Epochs')\n            plt.ylabel('Accuracy')\n            plt.ylim(0.0, 1.05)\n            plt.legend()\n            plt.grid(True)\n\n            plt.tight_layout()\n            plt.show()\n        else:\n            print(\"  Training history data is empty (training might have failed early).\")\n    else:\n         print(\"  No training history available for initial run.\")\nprint(\"=\"*60)\n\nlogging.info(\"--- Script Finished ---\")","metadata":{"_uuid":"a9461c03-9201-45dd-94df-1d76ab8e573d","_cell_guid":"3a9d2c47-9354-46de-9b2f-b50c5ddebdb2","trusted":true,"id":"wt0jduyHNfK-","executionInfo":{"status":"aborted","timestamp":1742977080529,"user_tz":-60,"elapsed":1,"user":{"displayName":"Khalil Krifi","userId":"13701701284808241864"}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Download result zip","metadata":{}},{"cell_type":"markdown","source":"This cell iterates through the `sorted_model_names` (which are the names of the models processed in the main loop). For each `model_name`:\n1.  It logs a message indicating that it's starting the checkpoint saving/uploading process for that model.\n2.  It calls the `upload_checkpoint_to_dataset(model_name)` function (defined in Cell 14). This function is responsible for:\n    *   Loading the saved metrics for the model.\n    *   Generating training history and confusion matrix plots and saving them in the model's checkpoint directory.\n    *   Packaging all relevant files (model weights, metrics JSON, plots) from the checkpoint directory into a zip file.\n    *   Handling the \"upload\" or saving of this zip file based on the environment:\n        *   **Kaggle**: Creates the zip in `/kaggle/working/` and provides download instructions/links.\n        *   **Colab**: Copies files to Google Drive.\n        *   **Local**: Copies files to a local directory.\n\nThe final comment suggests that after downloading the zip file(s), the user should extract them and then upload the results to a shared drive, implying a manual step for long-term storage or sharing.","metadata":{}},{"cell_type":"code","source":"for model_name in sorted_model_names:\n        logging.info(f\"\\n--- Saving/Uploading Checkpoints for {model_name} ---\")\n        upload_checkpoint_to_dataset( model_name)\n","metadata":{"_uuid":"10d88b09-35ad-4b1e-b97c-0802639b819d","_cell_guid":"4517a12c-0bc5-44ef-8a24-14ddfbc56b81","trusted":true},"outputs":[],"execution_count":null}]}